{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Myths about quantized neural networks\n",
    "> A post about misconceptions and myths around quantization of neural networks\n",
    "\n",
    "- toc: false \n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [jupyter, quantization]\n",
    "- image: images/gpu.jpg\n",
    "- author: Mathias Lechner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: What are quantized neural networks? \n",
    "\n",
    "When we run numerical algorithms on our computer we need to make sacrefices in terms of precision for the sake of runtime. \n",
    "For instance, the square root of 2 is an irrational number and has an infinite amount of decimal digits. \n",
    "Thus we need to decide how many digits we really need for our application.\n",
    "Each additional increases the memory and time requirements to store and compute a variable.\n",
    "\n",
    "For example, the IEEE-754 standard specifies four types of floaint-point formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.sqrt(2):\n",
      "float128: 1.4142135623730950488\n",
      "float64:  1.4142135623730951\n",
      "float32:  1.4142135\n",
      "float16:  1.414\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"np.sqrt(2):\")\n",
    "print(\"float128:\",str(np.sqrt(np.float128(2))))\n",
    "print(\"float64: \",str(np.sqrt(np.float64(2))))\n",
    "print(\"float32: \",str(np.sqrt(np.float32(2))))\n",
    "print(\"float16: \",str(np.sqrt(np.float16(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning applications, the ```float32``` format has been the default choice, as it provides a decent performance while avoiding extreme numerical errors. \n",
    "However, in the past decade researcher have made the following two observations:\n",
    "\n",
    "- During the training phase, certain types of layers can be run and trained with lower precision level (e.g. ```float16```)\n",
    "- After the training phase (=inference phase), neural networks can run with much lower precision levels without sacreficing much accuracy\n",
    "\n",
    "Consequently, Nvidia's latest [A100 GPU](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf) supports the following six numerical format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data type</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Significand precision</th>\n",
       "      <th>Exponent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>float64</td>\n",
       "      <td>Double precision IEE-754 floating-point</td>\n",
       "      <td>52-bits</td>\n",
       "      <td>11-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>float32</td>\n",
       "      <td>Single precision IEE-754 floating-point</td>\n",
       "      <td>23-bits</td>\n",
       "      <td>8-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TensorFloat32</td>\n",
       "      <td>32-bit floating-point format with reduced sign...</td>\n",
       "      <td>10-bits</td>\n",
       "      <td>8-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>float16</td>\n",
       "      <td>Half precision IEE-754 floating-point</td>\n",
       "      <td>10-bits</td>\n",
       "      <td>5-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfloat16</td>\n",
       "      <td>16-bit brain-float format with larger range bu...</td>\n",
       "      <td>7-bits</td>\n",
       "      <td>8-bits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>int8</td>\n",
       "      <td>8-bit integer format for fixed-point arithmetic</td>\n",
       "      <td>7-bits</td>\n",
       "      <td>0-bits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Data type                                            Comment  \\\n",
       "0        float64            Double precision IEE-754 floating-point   \n",
       "1        float32            Single precision IEE-754 floating-point   \n",
       "2  TensorFloat32  32-bit floating-point format with reduced sign...   \n",
       "3        float16              Half precision IEE-754 floating-point   \n",
       "4       bfloat16  16-bit brain-float format with larger range bu...   \n",
       "5           int8    8-bit integer format for fixed-point arithmetic   \n",
       "\n",
       "  Significand precision Exponent  \n",
       "0               52-bits  11-bits  \n",
       "1               23-bits   8-bits  \n",
       "2               10-bits   8-bits  \n",
       "3               10-bits   5-bits  \n",
       "4                7-bits   8-bits  \n",
       "5                7-bits   0-bits  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "df = pd.DataFrame({'Data type': [\"float64\"],\n",
    "        'TOPS': [9.7],\n",
    "        'Significand precision': ['52-bits'],\n",
    "        'Exponent': ['11-bits'],\n",
    "        'Comment': ['Double precision IEE-754 floating-point'],\n",
    "        })\n",
    "df = df.append({'TOPS': 19.5,\n",
    "        'Data type': \"float32\",\n",
    "        'Comment': 'Single precision IEE-754 floating-point',\n",
    "        'Significand precision': '23-bits',\n",
    "        'Exponent': '8-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 156,\n",
    "        'Data type': \"TensorFloat32\",\n",
    "        'Comment': '32-bit floating-point format with reduced significand precision',\n",
    "        'Significand precision': '10-bits',\n",
    "        'Exponent': '8-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 312,\n",
    "        'Data type': \"float16\",\n",
    "        'Comment': 'Half precision IEE-754 floating-point',\n",
    "        'Significand precision': '10-bits',\n",
    "        'Exponent': '5-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 312,\n",
    "        'Data type': \"bfloat16\",\n",
    "        'Comment': '16-bit brain-float format with larger range but reduced significand precision',\n",
    "        'Significand precision': '7-bits',\n",
    "        'Exponent': '8-bits',\n",
    "        },ignore_index=True)\n",
    "df = df.append({'TOPS': 624,\n",
    "        'Data type': \"int8\",\n",
    "        'Comment': '8-bit integer format for fixed-point arithmetic',\n",
    "        'Significand precision': '7-bits',\n",
    "        'Exponent': '0-bits',\n",
    "        },ignore_index=True)\n",
    "df[['Data type','Comment','Significand precision','Exponent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One item that stands out in this list is the last row: While all other formats are based on a floating-point representation, ```int8``` is an integer type.\n",
    "\n",
    "This raises the question: \n",
    "> **How can we run a neural network with integer operations?**\n",
    "\n",
    "The answer is quantization. \n",
    "\n",
    "> Important: Quantization turns a network that operates over floating-point variables into a network that uses fixed-point arithmetic\n",
    "\n",
    "[Fixed-point arithmetic](https://en.wikipedia.org/wiki/Fixed-point_arithmetic) is a numerical format that can be implemented relatively effeciently used integer operations.\n",
    "For instance, we can use the first four bits of an ```int8``` value to represent the digits before the comma, and the last four bits to represent fractional digits that come after the comma:\n",
    "```\n",
    "0.5       + 1.25      = 1.75\n",
    "0000.1000 + 0001.0100 = 0001.1100\n",
    "\n",
    "```\n",
    "A fixed-point addition can be implemented by simple integer addition and a fixed-point multiplication by a integer multiplication followed by an bit-wise shift operation.\n",
    "\n",
    "Obviously, the precision achieved with an 8-bit fixed-point format is not enough for training a neural network. However for inferencing, most types of layers can be quantized without suffering a significant loss in accuracy.\n",
    "The quantization step itself rounds the ```float32``` weight values to their nearest corresponding fixed-point value.\n",
    "\n",
    "The clear advantages of running a network using ```int8``` is that:\n",
    "\n",
    "1. It requies less memory, which improves cache and memory bandwidth efficiency.\n",
    "2. Can run using more efficient integer operations\n",
    "\n",
    "In particular, a [2017 Google paper](https://arxiv.org/pdf/1704.04760.pdf) writes:\n",
    "\n",
    "> Eight-bit integer multiplies can be 6X less energy and 6X less area than IEEE 754 16-bit floating-point multiplies, and the\n",
    "> advantage for integer addition is 13X in energy and 38X in area [Dal16].\n",
    "> \n",
    "> ***- ''In-Datacenter Performance Analysis of a Tensor Processing Unit'' - Jouppi et al.***\n",
    "\n",
    "\n",
    "Despite this relatively simple concept, there are several misconceptions and myths regarding quantized neural networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #1: Quantization is only necessary for ultra-low-power embedded systems\n",
    "\n",
    "Far from it. Data center applications currently benefit the most from quantization. \n",
    "For instance, the first generation of [Google's Tensor Processing Units (TPUs)](https://arxiv.org/pdf/1704.04760.pdf) only supported \n",
    "quantized networks. Support for floating-point arithmetic was only added in the [second generation](https://www.tomshardware.com/news/tpu-v2-google-machine-learning,35370.html).\n",
    "\n",
    "Likewise, Nvidia's [V100](https://www.microway.com/knowledge-center-articles/in-depth-comparison-of-nvidia-tesla-volta-gpu-accelerators/) and latest [A100](https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products) can\n",
    "perform four times as many ```int8``` tensor operations compared to ```float32``` operations per second (or twice as much ```int8``` as ```float16``` tensor operations per second).\n",
    "This means that, in a best case scenario, you can **quadruple the throughput of your data center application**\n",
    "if you quantize your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #2: Quantization makes networks smaller but not faster\n",
    "\n",
    "As already hinted in the myth above, modern AI accelerators such as GPU and TPU can run integer operations faster than floating-point operations.\n",
    "Let's have a look at the compute performance of Nvidia's latest [A100 GPU](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-0929ad37219d472d8838efe1ef3d5e7c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0929ad37219d472d8838efe1ef3d5e7c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0929ad37219d472d8838efe1ef3d5e7c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Data type\", \"legend\": null, \"scale\": {\"scheme\": \"dark2\"}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Data type\"}, {\"type\": \"quantitative\", \"field\": \"TOPS\"}, {\"type\": \"nominal\", \"field\": \"Comment\"}, {\"type\": \"nominal\", \"field\": \"Significand precision\"}, {\"type\": \"nominal\", \"field\": \"Exponent\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"labels\": false, \"title\": \"Data format\"}, \"field\": \"Data type\", \"sort\": [\"float64\", \"float32\", \"TensorFloat32\", \"float16\", \"bfloat16\", \"int8\"]}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Tera op/s\"}, \"field\": \"TOPS\"}}, \"height\": 300, \"title\": \"Nvidia A100 compute performance\", \"width\": 700}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -10, \"fontSize\": 16}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Data type\", \"legend\": null, \"scale\": {\"scheme\": \"dark2\"}}, \"text\": {\"type\": \"nominal\", \"field\": \"Data type\"}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"Data type\"}, {\"type\": \"quantitative\", \"field\": \"TOPS\"}, {\"type\": \"nominal\", \"field\": \"Comment\"}, {\"type\": \"nominal\", \"field\": \"Significand precision\"}, {\"type\": \"nominal\", \"field\": \"Exponent\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"labels\": false, \"title\": \"Data format\"}, \"field\": \"Data type\", \"sort\": [\"float64\", \"float32\", \"TensorFloat32\", \"float16\", \"bfloat16\", \"int8\"]}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Tera op/s\"}, \"field\": \"TOPS\"}}, \"height\": 300, \"title\": \"Nvidia A100 compute performance\", \"width\": 700}], \"data\": {\"name\": \"data-150720172dd484d891e26d6e7b27f0a1\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-150720172dd484d891e26d6e7b27f0a1\": [{\"Data type\": \"float64\", \"TOPS\": 9.7, \"Significand precision\": \"52-bits\", \"Exponent\": \"11-bits\", \"Comment\": \"Double precision IEE-754 floating-point\"}, {\"Data type\": \"float32\", \"TOPS\": 19.5, \"Significand precision\": \"23-bits\", \"Exponent\": \"8-bits\", \"Comment\": \"Single precision IEE-754 floating-point\"}, {\"Data type\": \"TensorFloat32\", \"TOPS\": 156.0, \"Significand precision\": \"10-bits\", \"Exponent\": \"8-bits\", \"Comment\": \"32-bit floating-point format with reduced significand precision\"}, {\"Data type\": \"float16\", \"TOPS\": 312.0, \"Significand precision\": \"10-bits\", \"Exponent\": \"5-bits\", \"Comment\": \"Half precision IEE-754 floating-point\"}, {\"Data type\": \"bfloat16\", \"TOPS\": 312.0, \"Significand precision\": \"7-bits\", \"Exponent\": \"8-bits\", \"Comment\": \"16-bit brain-float format with larger range but reduced significand precision\"}, {\"Data type\": \"int8\", \"TOPS\": 624.0, \"Significand precision\": \"7-bits\", \"Exponent\": \"0-bits\", \"Comment\": \"8-bit integer format for fixed-point arithmetic\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "points = alt.Chart(df).mark_bar().encode(\n",
    "    alt.X('Data type',sort=list(df['Data type']),axis=alt.Axis(labels=False,title='Data format')),\n",
    "    y=alt.Y('TOPS',axis=alt.Axis(title=\"Tera op/s\")),\n",
    "    color=alt.Color('Data type',legend=None,scale=alt.Scale(scheme='dark2')),\n",
    "    tooltip=['Data type','TOPS','Comment', 'Significand precision','Exponent']\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=300,\n",
    "    title=\"Nvidia A100 compute performance\"\n",
    ")\n",
    "\n",
    "#points\n",
    "text = points.mark_text(\n",
    "    align='center',\n",
    "    fontSize=16,\n",
    "    baseline='middle',\n",
    "    dy=-10\n",
    ").encode(\n",
    "    text='Data type'\n",
    ")\n",
    "points + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, quantization does not only make the network smaller, but it also runs faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #3: Any layer in a neural network can be quantized\n",
    "\n",
    "Some types of layers do not tolerate quantization very well. For example, in [a discussion of an ICLR paper by Max Welling's group](https://openreview.net/forum?id=HkxjYoCqKX&noteId=rygmk1EDT7) we see that quantizing the first or the last layer of a network results an a considerable drop in accuracy. \n",
    "This gap does not completely close even if we train the network using quantization-aware training.\n",
    "\n",
    "One trick which is often used to avoid this drop in accuracy is to not quantize the first and the last layer. \n",
    "As these two layers only take up a small fraction of the computations inside a network, running the first and the last layer with ```float32``` does not hurt throughput much, but significantly benefits the accuracy of the network. \n",
    "\n",
    "However, one some end-devices this approach is not an option. For instance, [Google's Edge TPU](https://cloud.google.com/edge-tpu) only supports ```int8```. Therefore, in such cases every layer of the network must be quantized to 8-bit integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #4: It's easy to compare different quantization approaches\n",
    "\n",
    "Comparing two different quantization method is not a trivial job. \n",
    "Connecting to the discussion above, let's imagine we have a network and quantize it with two different methods to obtain network A and network B.\n",
    "While, network A achieves a 90% accuracy by quantized all layers, network B achieves a 92% accuracy but leaves the first layer running with floating-point precision.\n",
    "\n",
    "Which method is better?\n",
    "\n",
    "The answer of this question depends on the context; which target device will the network run on?\n",
    "If it's a device wihtout a floaing-point unit such as the Edge TPU or a microcontroller, then method A is clearly better. \n",
    "Contrarily, if we plan to run the network on a V100 or A100 GPU, then method B might be the better approach.\n",
    "\n",
    "Another technique that causes a lot of misconceptions found in [the discussion of the ICLR paper by Max Welling's group](https://openreview.net/forum?id=HkxjYoCqKX&noteId=rygmk1EDT7) are **non-uniform quantization schemes**:\n",
    "Fixed-point formats partition the representable value range using a uniform grid, e.g., there are the same amount of intermediate values between 1.5 and 2.5 as between 2.5 and 3.5. \n",
    "Looking at the typical weight distribution of neural networks, we notice that they follow a Gaussian-like bell curve distribution with smaller values occurcing more frequently than large weight values. \n",
    "\n",
    "If our sole purpose is to represent the weight values as accurately as possible using only 8-bits per parameter, then a logarithmic grid provides a better choice than the uniform partitioning of fixed-point formats.\n",
    "Although, such non-uniform quantization approaches can compress the weights of the network remarkably well, they destroy the key advantage of quantization: The use of integer arithemtic.\n",
    "\n",
    "In essence, comparing quantization method can be quite challenging as there are many tradeoffs to be considered. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Myth #5: A 8-bit network uses only 8-bit variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
