{
  
    
        "post0": {
            "title": "5 Myths about quantized neural networks",
            "content": "Background: What are quantized neural networks? . When we run numerical algorithms on our computer, we need to make sacrifices in terms of precision for the sake of runtime. For instance, the square root of 2 is an irrational number and has an infinite amount of decimal digits. Thus we need to decide how many digits we really need for our application. Each extra digit of precision increases the memory and time requirements to store and compute a variable. . For example, the IEEE-754 standard specifies four types of floating-point formats: . np.sqrt(2): float128: 1.4142135623730950488 float64: 1.4142135623730951 float32: 1.4142135 float16: 1.414 . For machine learning applications, the float32 format has been the default choice, as it provides a decent performance while avoiding extreme numerical errors. However, in the past decade researcher have made the following two observations: . During the training phase, certain types of layers can be run and trained with lower precision (e.g., float16) | After the training phase (=inference phase), neural networks can run with much lower precision levels without sacrificing much accuracy | . Consequently, Nvidia&#39;s latest A100 GPU supports the following six numerical format: . Data type Significand precision Exponent . 0 float64 | 52-bits | 11-bits | . 1 float32 | 23-bits | 8-bits | . 2 TensorFloat32 | 10-bits | 8-bits | . 3 float16 | 10-bits | 5-bits | . 4 bfloat16 | 7-bits | 8-bits | . 5 int8 | 7-bits | 0-bits | . One item that stands out in this list is the last row: While all other formats are based on a floating-point representation, int8 is an integer type. . This raises the question: . How can we run a neural network with integer operations? . The answer is quantization. . Quantization translates a network that operates over floating-point variables into a network that uses fixed-point arithmetic . Fixed-point arithmetic is a numerical format that can be implemented relatively efficiently used integer operations.For instance, we can use the first four bits of an int8 value to represent the digits before the comma, and the last four bits to represent fractional digits that come after the comma: . Decimal: 0.5 + 1.25 = 1.75 Binary: 0000.1000 + 0001.0100 = 0001.1100 . A fixed-point addition can be implemented by simple integer addition and a fixed-point multiplication by an integer multiplication followed by a bit-wise shift operation. . Obviously, the precision achieved with an 8-bit fixed-point format is not enough for training a neural network. However, most types of layers can be quantized for inferencing without suffering a significant loss in accuracy. The quantization step itself rounds the float32 weight values to their nearest corresponding fixed-point value. . The clear advantages of running a network using int8 is that: . It requires less memory, which improves cache and memory bandwidth efficiency. | Can run using more efficient integer operations | In particular, a 2017 Google paper writes: . Eight-bit integer multiplies can be 6X less energy and 6X less area than IEEE 754 16-bit floating-point multiplies and the advantage for integer addition is 13X in energy and 38X in area [Dal16]. . - &#39;&#39;In-Datacenter Performance Analysis of a Tensor Processing Unit&#39;&#39; - Jouppi et al. . Despite this relatively simple concept, there are several misconceptions and myths regarding quantized neural networks: . Myth #1: Quantization is only necessary for ultra-low-power embedded systems . Far from it. Datacenter applications currently benefit the most from quantization. For instance, the first generation of Google&#39;s Tensor Processing Units (TPUs) only supported quantized networks. Computation units for floating-point arithmetic were only added in the second generation. . Likewise, Nvidia&#39;s V100 and latest A100 can perform four times as many int8 tensor operations compared to float32 operations per second (or twice as much int8 as float16 tensor operations per second). This means that you can quadruple the throughput of your datacenter application with quantization in a best-case scenario. . Myth #2: Quantization makes networks smaller but not faster . As already hinted in the myth above, modern AI accelerators such as GPU and TPU can run integer operations faster than floating-point operations. Let&#39;s look at the compute performance of Nvidia&#39;s latest A100 GPU: . Essentially, quantization does not only make the network smaller, but makes them also runs faster! . Myth #3: Any layer in a neural network can be quantized . Some types of layers do not tolerate quantization very well. For example, in a discussion of an ICLR paper by Max Welling&#39;s group we see that quantizing the first or the last layer of a network results in a considerable drop in accuracy. This gap does not entirely close even if we train the network using quantization-aware training techniques. . One trick often used to avoid this drop in accuracy is not to quantize the first and the last layer. As these two layers only take up a small fraction of the computations inside a network, running the first and the last layer with float32 does not hurt throughput much, but significantly benefits the accuracy of the network. . However, one some end-devices, this approach is not an option. For instance, Google&#39;s Edge TPU only supports int8. Therefore, in such cases, every layer of the network must be quantized to 8-bit integers. . Myth #4: It&#39;s easy to compare different quantization approaches . Comparing two different quantization methods is not a trivial job. Connecting to the discussion above, let&#39;s imagine we have a network and quantize it with two different methods to obtain network A and network B. While network A achieves a 90% accuracy by quantized all layers, network B achieves a 92% accuracy but leaves the first layer running with floating-point precision. . Which method is better? . The answer to this question depends on the context; which target device will the network run on? If it&#39;s a device without a floating-point unit such as the Edge TPU or a microcontroller, then method A is clearly better. Contrarily, if we plan to run the network on a V100 or A100 GPU, then method B might be the better approach. . Another technique that causes a lot of misconceptions found in the discussion of the ICLR paper by Max Welling&#39;s group are non-uniform quantization schemes: Fixed-point formats partition the representable value range using a uniform grid, e.g., there are the same amount of intermediate values between 1.5 and 2.5 as between 2.5 and 3.5. Looking at the typical weight distribution of neural networks, we notice that they follow a Gaussian-like bell curve distribution with smaller values occurring more frequently than large weight values. . If our only objective is to represent the weight values as accurately as possible using only 8-bits per parameter, then a logarithmic grid provides a better choice than the uniform partitioning of fixed-point formats. Although such non-uniform quantization approaches can remarkably compress the weights of the network, they destroy the critical advantage of quantization: The enabling of efficient integer arithmetic. . In essence, comparing different quantization methods can be quite challenging as there are many tradeoffs to be considered. . Myth #5: A 8-bit network uses only 8-bit variables . In an 8-bit quantized neural network, all weights and activations are 8-bit integer variables. However, when we run the network, we need 32-bits accumulation registers internally to represent intermediate results. . Let&#39;s take a look at the C-like implementation of a typical ReLU neuron: . float relu_neuron(float* w, float* x, size_t n){ float accumulator = 0; for(size_t i=0;i&lt;n;i++){ accumulator += w[i]*x[i]; } accumulator = max(0,accumulator); return accumulator; } . Essentially all variables, the weights, the inputs, and the accumulator, are represented by the 32-bit floating-point type. For a neural network in a fixed-point format, we need a wider integer accumulation register to perform the multiplications and summation. . int8_t relu_neuron(int8_t* w, int8_t* x, size_t n, int shift){ int32_t accumulator = 0; for(size_t i=0;i&lt;n;i++){ // Multiplying two 8-bit integers requires a 16-bit register int16_t product = w[i]*x[i]; // Sum over several 16-bit integers requires an even larger register (e.g. int32_t) accumulator += product; } // Fixed-point multiplication requires a shift accumulator = accumulator&gt;&gt;shift; accumulator = max(0,accumulator); // Project neuron value back to valid int8_t range accumulator = min(INT8_MAX,accumulator); // Cast 32-bit accumulator back to 8-bit integer return (int8_t)accumulator; } . Note that the parameter shift depends on the exact fixed-point format we employ, i.e., how many bits we use for storing the digits before and how many bits we use for storing the digits after the comma. . Page 27 of Nvidia&#39;s A100 whitepaper specifies precisely which numerical format uses which numerical type for representing the input operands and the accumulation registers. . Conclusion . Although the concept of neural network quantization is relatively simple, there are subtle details that can cause misconceptions. In this post, we have busted five common myths regarding quantized neural networks. .",
            "url": "https://mlech26l.github.io/pages/jupyter/quantization/2020/07/18/quantization.html",
            "relUrl": "/jupyter/quantization/2020/07/18/quantization.html",
            "date": " â€¢ Jul 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "We live in a Teraflop era",
            "content": "OpenAI recently released a blog post, showing that the advances in algorithmic efficiency for training neural nets outpaced the scaling of Moore&#39;s law. In particular, the amount of transistors in silicon chips doubles every two years, whereas the efficiency of training a neural net to a certain accuracy level doubles every 16 to 17 months. While this progress is impressive, I am arguing that Moore&#39;s law is one of the main drivers of machine learning research. Thus, the advances in neural nets training efficiency are built on the shoulders of Moore&#39;s law. In essence, I am stating the following law: . The amount of architecture tuning that can be done for a fixed budget doubles every two years . Moore&#39;s Law - Machine Learning Research Edition . By budget I am refering to time, money and compute resources. My definition of architecture involves any non-weight parameter/decision and includes the network design, hyperparameters, and training method as well, (e.g., self-supervised pre-training). . Machine Learning Research . If we look at the methodology of machine learning research, we notice an iterative paradigm composed of the following steps. . We have an idea | We test the idea | Based on the results, we refine and improve the idea. | Once this iterative process yields noteworthy results, the idea and corresponding test results get distilled into a research paper. . Let&#39;s say we want to speed up our research. Common sense tells us that we can speed up any process by getting rid of its bottlenecks. The most dominant bottleneck in the procedure above is obviously step number 2. . We could run more machine learning experiments if we simply buy a larger quantity of faster compute units. But what if our budget is limited? How can we speed up our research then? . The answer is simply waiting. Yes! Moore&#39;s law tells us that every 2 years, we get roughly twice the compute performance for the same budget. For instance, here is a plot of how the 32-bit floating-point of Nvidia GPU performance increased in the past decade: . This chart does not even include the improvements of mixed-precision methods and other tricks that achieve higher performance by sacrificing numerical precision. For instance, Nvidia&#39;s latest A100 can perform 156 Teraflop/s when using for machine learning dedicated TensorCores together with the slightly less precise TensorFloat32 numerical format. If we compare the TensorFloat32 throughput of the A100 to the GTX 580 used by Alex Krizhevsky to train AlexNet, we see a 100x scaling in compute performance in almost exactly ten years (or roughly 10x if we compare full-precision float32 performance). . But what about larger models and datasets? . Of course, the statement about the doubling of the architecture tuning assumes that the datasets and extend of the networks does not change dramatically. However, given that ImageNet is still the de-facto standard computer vision benchmark (special credit to Prof. Fei-Fei), and the fact that the top-performing networks in 2019/2020 (EfficientNet) are smaller than their 2015/2016 counterparts (ResNet/ResNeXt), this assumption seems to hold. . Conclusion . Moore&#39;s law continues to make compute resources faster and cheaper. This increase in compute performance allows machine learning researchers to test a larger variety of new architecture and training methods. Therefore, I am expecting that research will continue to yield better neural archtectiures and training algorithms. .",
            "url": "https://mlech26l.github.io/pages/jupyter/2020/07/18/gpu.html",
            "relUrl": "/jupyter/2020/07/18/gpu.html",
            "date": " â€¢ Jul 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The dangers of biases in AI",
            "content": "Background . At the 2020 Conference on Computer Vision and Pattern Recognition (CVPR) Duke University published a new method title Photo Upsampling via Latent Space Exploration (PULSE) for upsampling low resolution photos. In a nutshell, PULSE searches the latent space of a generative model to find a high-resolution image that downsamples to an image that looks similar to the low-resolution input image. In particular, the authors achieved fascinating looking results by using StyleGAN trained on the FFHQ dataset of high-resolution images of human faces. . Indeed, if we run PULSE with downsampled images generated by StyleGAN itself, we get remarkable detailed results: . . Nonetheless, the method has sparked extensive discussions about biases in AI. It all started with a tweet showing how PULSE upscaled a low-res picture of Barack Obama to a tanned white person. . ðŸ¤”ðŸ¤”ðŸ¤” pic.twitter.com/LG2cimkCFm . &mdash; Chicken3gg (@Chicken3gg) June 20, 2020 The authors explicitly state that the objective of PULSE is not to reconstruct the original image, as this is impossible due to the lack of information in the low-res image. Nonetheless, one would never expect the low-res image of Barack Obama to be upscaled to the picture shown above. . The authors claim that biased training data of StyleGAN most likely causes the observed bias. The used FFHQ dataset primarily comprises of faces of white people. They hypothesize that, due to this training data imbalance, the explored parts of the latent space by PULSE correspond predominately to white peopleâ€™s faces. . The dangers of biased AI . Despite this claimed lack of training data diversity, whatâ€™s fascinating and dangerous at the same time is how this bias manifests in the AI. To examine this, letâ€™s downsample the Dancing Pallbearers and see what images PULSE will generate. . . While the synthetic low-res images share color and shape profile of the corresponding real picture, we notice a crucial property in each upsample image: Itâ€™s always a white person. Surprisingly, the AI comes up with subtle reasons for explaining the skin tone and head covering in the input image. The skin tone is not caused by dark skin color but because the person in the synthetic image stands in a shaded area. The white stripes on the hats are explained by sun rays hitting on that part of the personâ€™s face. Moreover, the black area from the top hat in the first image is explained by the high volume of the personâ€™s hair. . We see that the AI finds creative ways to explain the features observed in the test images that werenâ€™t there during training. This is precisely where the danger of biased AI systems lie. The AI can find solutions that are valid on a mathematical level (L2-distance) but unacceptable on a societal level. We can repeat the experiments, but each time PULSE comes up with the same explanations. . . Bias beyond mathematics . The authors of the original paper performed an additional experiment to evaluate the &#39;â€™success rateâ€™â€˜ of PULSE of various groups (Female/Male, Black, East Asian, Indian, etc. ). Their definition of success only concerns if the method finds an image that downsamples to a similar-looking image as the low-resolution input with respect to a pixel-based distance metric or not. The authors found a marginal but non-significant difference in â€˜â€˜success rateâ€™â€™ among the tested groups. Essentially, as I stated above, on a mathematical level, the authors were unable to characterize a significant bias in the system. . Conclusion . Bias in machine learning is a sensitive topic that cannot be reduced to mathematics and training data imbalance alone. It mandates examinations involving societal and domain-specific experts. .",
            "url": "https://mlech26l.github.io/pages/2020/06/30/biases.html",
            "relUrl": "/2020/06/30/biases.html",
            "date": " â€¢ Jun 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Distinguished Young Alumnus-Award",
            "content": "I won the Distinguished Young Alumnus-Award at the Epilog for my master thesis Brain-inspired Neural Control. The award was given by the TU Wien faculty of Informatics and is endowed 1.500 EUR. . . Remark: The research paper of my master thesis has been published at the 2019 IEEE International Conference on Robotics and Automation (ICRA) .",
            "url": "https://mlech26l.github.io/pages/2018/01/20/epilog.html",
            "relUrl": "/2018/01/20/epilog.html",
            "date": " â€¢ Jan 20, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Tikz and Videos",
            "content": "I am a really big fan of Vector graphics and I exhaustively use Tikz to draw such. However, creating videos and cool animations with Tikz and Latex can be a bit messy. So I have come up with a simple but effective hack to make generating videos with Latex easier. . The key idea is to have two separated Latex files: One that is manually created and stays the same over the whole video, and one that is procedurally generated for each frame. The manual one is created as you would do usually write your Latex image. However, all variables that are dynamic throughout the video (e.g. color, position, opacity, etc.) are defined by placeholders. And, guess what, the second Latex file (the procedurally generated one) then fills all the placeholder for each frame. . Drawing a frame . My starting point is the manually designed Latex file, that is based on the following scheme: . documentclass[border=0cm,convert={outext=.png}]{standalone} % Documentclass to directly create a PNG-files when invoking &#39;pdflatex&#39; usepackage{xcolor} usepackage{tikz} % Load dynamic variables input{dynamic.tex} begin{document} begin{tikzpicture} % My Latex code node (start) at (0,0) [draw,fill= mycolorsin] {A}; node (start) at (2,0) [draw,fill= mycolorcos] {B}; end{tikzpicture} end{document} . In this case a two nodes are drawn, which colors ( mycolor) should be animated. . . Filling the placeholders . In the next step I will create a simple python script that generates the dynamic.tex file and fills all the dynamic placeholder variables. Moreover, the program invokes the Latex compiler to generate a PNG file out of the code and stores it in a directory. . import numpy as np import time import os import subprocess import shutil # Create directory for the frames if not os.path.exists(&#39;sequence&#39;): os.makedirs(&#39;sequence&#39;) # Example data for animation in range [0,100] t = np.linspace(0,2*np.pi,100) sin_seq = 50.0*np.sin(t)+50.0 cos_seq = 50.0*np.cos(t)+50.0 # Loop over each frame for i in range(100): # Create file with for dynamic variables dynamic_file = open(&#39;dynamic.tex&#39;,&#39;w&#39;) dynamic_file.write(&#39; newcommand{ mycolorsin}{green!&#39;+str(int(round(sin_seq[i])))+&#39;!white} n&#39;) dynamic_file.write(&#39; newcommand{ mycolorcos}{red!&#39;+str(int(round(cos_seq[i])))+&#39;!white} n&#39;) dynamic_file.close() # Create png with pdflatex os.system(&#39;pdflatex -shell-escape -interaction=nonstopmode example.tex&#39;) # Move frame into directory with frames shutil.move(&#39;example.png&#39;, &#39;sequence/frame_&#39;+str(i).zfill(3)+&#39;.png&#39;) . Finally, all I have to do is to animate the frames with ImageMagick . convert -loop 0 -delay 2 sequence/*.png animation.gif . . Remarks . Instead of using newcommand for each placeholder, it might be more convinient to use tikzset or tikzstyle (See here) to define the dynamic variables | If you want to create a video instead of an animation (mp4 instead of gif), you can easily do this with ffmpeg | Technically the gif animation is not a vector graphics anymore. However by tuning the pdf-to-png conversion density parameter, one can create an animation with an arbitrary high resolution. | .",
            "url": "https://mlech26l.github.io/pages/2017/11/29/tikz_and_videos.html",
            "relUrl": "/2017/11/29/tikz_and_videos.html",
            "date": " â€¢ Nov 29, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am a PhD candidate at IST Austria at the research group of Tom Henzinger. My research topics are Machine Learning, Formal Methods, and Robotics. . Publications . ICML (2020) The Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits. Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu. | IJCAR (2020) An SMT Theory of Fixed-Point Arithmetic. Marek Baranowski, Shaobo He, Mathias Lechner, Thanh Son Nguyen, and Zvonimir Rakamaric [Paper] | ICLR (2020) Learning Representations for binary classification without backpropagation. Mathias Lechner. [Paper] | ICRA (2020) Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-To-End Robot Learning Scheme. Mathias Lechner, Ramin Hasani, Daniela Rus, and Radu Grosu. [Paper] | TACAS (2020) How Many Bits Does it Take to Quantize Your Neural Network?. Mirco Giacobbe, Thomas A. Henzinger, and Mathias Lechner [Paper] | ICRA (2019) Designing Worm-inspired Neural Networks for Interpretable Robotic Control. Mathias Lechner, Ramin Hasani, Manuel Zimmer, Thomas A. Henzinger, and Radu Grosu [Paper] | . Awards . Distinguished Young Alumnus-Award 2018 Faculty of Informatics at TU Wien .",
          "url": "https://mlech26l.github.io/pages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}