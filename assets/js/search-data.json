{
  
    
        "post0": {
            "title": "",
            "content": "OpenAI recently release a blog post showing that the advances in algorithmic efficiency for training neural nets outpaced the scaling of Moore&#39;s law. In particular, the amount of transistors in silicon chips doubles every 2 years, whereas the efficiency of training a neural net to a certain accuracy level doubles every 16 to 17 months. While this progess is impressive, I am arguing here that Moore&#39;s law is one of the main drivers in machine learning research. Thus, the advances in neural nets training efficiency is built on the shoulders of Moore&#39;s law. In essence, I am stateing the following law: . The amount of hyperparameter &amp; architecture tuning that can be done for a fixed budget doubles every 2 years . Moore&#39;s Law:Machine Learning Research Edition By budget I mean time, money and compute resources. . Machine Learning Research . If we look at the methodology of machine learning research we notice an iterative paradigm composed of the following steps . We have an idea | We test the idea | Based on the results we refine and improve the idea. | Once this iterative process yields noteworthy results, the idea and corresponding test results get distilled into a research paper. . Let&#39;s say we want to speed up our research. Common sense tells us that we can speed up any process by getting rid of its bottlenecks. The most domniant bottleneck in the procedure above is obviously step 2. . We could run more machine learning experiments if we simply buy a larger quanitity of faster compute units. But what if our budget is limited? How can we speed up our research then? . The answer is, by simply waiting. Yes! Moore&#39;s law tells us that in 2 years we get roughly twice the compute performance for the same budget. For instance, here is a plot of how the 32-bit floating-point of Nvidia GPU performance increased in the past decade: . This chart does not even include the improvements of Mixed-precision methods and other tricks that achieve a higher performance by sacreficing arithmetic precision. For instance, Nvidia&#39;s latest A100 can perform 156 Teraflop/s when using the slightly less precise TensorFloat32 format. If we compare the TensorFloat32 throughput of the A100 to the GTX 580 used by Alex Krizhevsky to train AlexNet, we see a 100x jump in compute performance in almost exactly 10 years. . But what about larger models and datasets? . Of course the statement about the doubling of the hyperparameter &amp; architecture tuning assumes that the datasets and extend of the networks does not change dramatically. However, given that ImageNet is still the de-facto standard computer vision benchmark (special credit to Prof. Fei-Fei), and the fact that the top performing networks in 2020 (EfficientNet) are smaller than their 2016 counterparts (ResNet/ResNeXt), this assumptions holds. .",
            "url": "https://mlech26l.github.io/pages/2020/06/07/2020-06-06-gpu.html",
            "relUrl": "/2020/06/07/2020-06-06-gpu.html",
            "date": " • Jun 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Distinguished Young Alumnus-Award",
            "content": "I won the Distinguished Young Alumnus-Award at the Epilog for my master thesis Brain-inspired Neural Control. The award was given by the TU Wien faculty of Informatics and is endowed 1.500 EUR. . .",
            "url": "https://mlech26l.github.io/pages/2018/01/20/epilog.html",
            "relUrl": "/2018/01/20/epilog.html",
            "date": " • Jan 20, 2018"
        }
        
    
  
    
        ,"post2": {
            "title": "Tikz and Videos",
            "content": "I am a really big fan of Vector graphics and I exhaustively use Tikz to draw such. However, creating videos and cool animations with Tikz and Latex can be a bit messy. So I have come up with a simple but effective hack to make generating videos with Latex easier. . The key idea is to have two separated Latex files: One that is manually created and stays the same over the whole video, and one that is procedurally generated for each frame. The manual one is created as you would do usually write your Latex image. However, all variables that are dynamic throughout the video (e.g. color, position, opacity, etc.) are defined by placeholders. And, guess what, the second Latex file (the procedurally generated one) then fills all the placeholder for each frame. . Drawing a frame . My starting point is the manually designed Latex file, that is based on the following scheme: . documentclass[border=0cm,convert={outext=.png}]{standalone} % Documentclass to directly create a PNG-files when invoking &#39;pdflatex&#39; usepackage{xcolor} usepackage{tikz} % Load dynamic variables input{dynamic.tex} begin{document} begin{tikzpicture} % My Latex code node (start) at (0,0) [draw,fill= mycolorsin] {A}; node (start) at (2,0) [draw,fill= mycolorcos] {B}; end{tikzpicture} end{document} . In this case a two nodes are drawn, which colors ( mycolor) should be animated. . . Filling the placeholders . In the next step I will create a simple python script that generates the dynamic.tex file and fills all the dynamic placeholder variables. Moreover, the program invokes the Latex compiler to generate a PNG file out of the code and stores it in a directory. . import numpy as np import time import os import subprocess import shutil # Create directory for the frames if not os.path.exists(&#39;sequence&#39;): os.makedirs(&#39;sequence&#39;) # Example data for animation in range [0,100] t = np.linspace(0,2*np.pi,100) sin_seq = 50.0*np.sin(t)+50.0 cos_seq = 50.0*np.cos(t)+50.0 # Loop over each frame for i in range(100): # Create file with for dynamic variables dynamic_file = open(&#39;dynamic.tex&#39;,&#39;w&#39;) dynamic_file.write(&#39; newcommand{ mycolorsin}{green!&#39;+str(int(round(sin_seq[i])))+&#39;!white} n&#39;) dynamic_file.write(&#39; newcommand{ mycolorcos}{red!&#39;+str(int(round(cos_seq[i])))+&#39;!white} n&#39;) dynamic_file.close() # Create png with pdflatex os.system(&#39;pdflatex -shell-escape -interaction=nonstopmode example.tex&#39;) # Move frame into directory with frames shutil.move(&#39;example.png&#39;, &#39;sequence/frame_&#39;+str(i).zfill(3)+&#39;.png&#39;) . Finally, all I have to do is to animate the frames with ImageMagick . convert -loop 0 -delay 2 sequence/*.png animation.gif . . Remarks . Instead of using newcommand for each placeholder, it might be more convinient to use tikzset or tikzstyle (See here) to define the dynamic variables | If you want to create a video instead of an animation (mp4 instead of gif), you can easily do this with ffmpeg | Technically the gif animation is not a vector graphics anymore. However by tuning the pdf-to-png conversion density parameter, one can create an animation with an arbitrary high resolution. | .",
            "url": "https://mlech26l.github.io/pages/2017/11/29/tikz_and_videos.html",
            "relUrl": "/2017/11/29/tikz_and_videos.html",
            "date": " • Nov 29, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am a PhD candidate at IST Austria at the research group of Tom Henzinger. My research topics are Machine Learning, Formal Methods, and Robotics. . Publications . ICML (2020) The Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits | IJCAR (2020) An SMT Theory of Fixed-Point Arithmetic | ICLR (2020) Learning Representations for binary classification without backpropagation | ICRA (2020) Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-To-End Robot Learning Scheme | TACAS (2020) How Many Bits Does it Take to Quantize Your Neural Network? | ICRA (2019) Designing Worm-inspired Neural Networks for Interpretable Robotic Control | . Awards . Distinguished Young Alumnus-Award 2018 Faculty of Informatics at TU Wien .",
          "url": "https://mlech26l.github.io/pages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}