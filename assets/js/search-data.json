{
  
    
        "post0": {
            "title": "The wormnet project",
            "content": "This is the first part in a series explaining our recent paper titled XXX. . In progress. . Biological vs artificial neural networks . The term neural implies the origin of neural networks lies in biological nervous systems. Indeed, artificial neural networks’ original purpose was to build intelligent algorithms inspired by how information processing happens in human and animal brains. However, modern neural networks for building powerful machine learning models look quite distinct compared to their biological counterparts. There are two critical differences between artificial and biological neural networks: the computational model and the architecture. . . Biological neurons are cells whose cell-wall have an electrical charge depending on the concentration of certain ions inside vs. outside the cell. The number of ions inside the cell can be changed by mechanisms of the cell itself when the cell’s electrical charge exceeds a threshold or via neurotransmitters released by synapses coming from other neurons. On the other side of the spectrum, an artificial neuron is simply a weighted summation of its inputs, followed by a non-linear function. This high-level abstraction is an extreme simplification of the underlying information processing mechanism. On the one hand, this simplification lets us easily design deep neural networks of millions of units with only a few code lines. One the other hand, it raises the question of whether we lose some important properties and aspects of biological neurons. . The second major difference between artificial and biological neural networks is their wiring architectures. Biological neural networks appear chaotic at first glance and require extensive research to understand and characterize their wiring principles. Artificial neural networks have a well-defined layer-by-layer structure, which is often guided by computational reasons. For instance, cache efficient dense matrix multiplication algorithms make the use of fully-connected layers very convenient. Consequently, writing a fully-connected layer in Pytorch or TensorFlow requires fewer code lines than creating a sparsely connected layer. . Given the two discrepancies between artificial and biological neural networks mentioned above, we ask the following question in our research: What do we gain something, and what costs do we have to pay if we design more biologically inspired machine learning models? . But there are already a million papers out there asking similar questions, what’s so special about your research? . That’s correct. But different from other research out there, we don’t look at human brains for our inspirations. We don’t even look at rats or mice brains for our comparisons. Heck, we don’t even consider the brains of insects or fish for our study. We look at the arguably simplest animal with a functioning nervous system out there: The Caenorhabditis elegans worm. . . From a modeling viewpoint, the C. elegans organism has two key advantages: C. elegans’ nervous system consists of only 302 neurons and around 9000 synapses, and its wiring is well-studied 1. The neurons of C. elegans’ nervous system don’t express any spiking patterns, which makes modeling them much simpler. . In our research, i.e., the wormnet project, we try to build machine learning models motivated by the C. elegans nervous system. By doing so, we have to pay a cost, as we constrain ourselves to such models in contrast to standard artificial neural networks, whose modeling space is purely constraint by memory and compute limitations. However, there are potentially some advantages and benefits we gain. Our objective is to better understand what’s necessary for effective neural information processing to emerge. . References . wormatlas.org &#8617; . |",
            "url": "https://mlech26l.github.io/pages/2020/09/14/wormnet1.html",
            "relUrl": "/2020/09/14/wormnet1.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "The model",
            "content": "This is the second part in a series explaining our recent paper titled XXX. . Neurons are cells . As briefly discussed in the first part, the neuron model used in machine learning is quite a high-level abstraction of the complex cells neuroscience deals with. We have also talked about how the activation value of a neuron is actually an electrical potential caused by different concentrations of charged ions inside and outside the cell. Going one step further, there may be different ion concentrations at different locations inside the cell, resulting in the neuron not having a single activation value but a location-dependent potential. We won’t go this far of construction such a compartmental model, but assume a neuron has a single electrical potential. We start by modeling the neuron’s membrane that separates the ions inside and outside the cell as a capacitor. . Cdvdt=i(t)C frac{d v}{d t} = i(t)Cdtdv​=i(t) . The voltage the capacitor is charged to represent the activation of the neuron. The idea of modeling a neuron’s membrane as a capacitor dates back to 1907 1 and is used is many influencial neuron models 2. Next, we assume the neuron has a resting potential: the potential it wants to attain when no external stimulus is applied to the neuron. . . A network of such standalone RC-circuits is quite boring, so we need a synapse model to let the neurons communicate with each other. Physical synapses trigger an increase of certain ions in the post-synaptic neuron through neurotransmitters released by the pre-synaptic neuron. As a result, the post-synaptic neuron’s potential increases or decreases, depending on the ions’ flow direction and polarity. The release of neurotransmitters itself is triggered by pre-synaptic neuron’s potential exceeding a certain threshold. We model this process by adding the following electrical circuit to the post-synaptic neuron, . . where Erev,E_{rev,}Erev,​ determines the reverse-potential, i.e., the potential toward which the synapses pushes the post-synaptic neuron’s potential. In the case of an excitatory synapse, this value is larger than the resting potential (activating the neuron). In the case of an inhibitory synapse, Erev,kE_{rev,k}Erev,k​ is lower than the resting potential (deactivating the neuron). . The interesting non-linear dynamics of the synapse happens inside the varistor gkg_{k}gk​ . The conductance of it is governed by the equation . gk=Gk1+exp⁡(−σvpre,k+μk)g_k = frac{G_k}{1+ exp{(- sigma v_{pre,k} + mu_{k})}}gk​=1+exp(−σvpre,k​+μk​)Gk​​ . , where vpre,kv_{pre,k}vpre,k​ is the pre-synaptic neuron’s potential, Gk,σk,μkG_k, sigma_k, mu_kGk​,σk​,μk​ weights of the synapse. . Now that we have a neuron model and a synapse model to let the neurons talk to each other, we still miss a critical part of a neural network: the inputs and outputs. In C. elegans’ nervous system, physical inputs are introduced into the network through sensory neurons, i.e., neurons whose potential is influenced by external stimulus. Analogously, dedicated motor neurons take care of producing a physical response from the information processing inside C. elegans’ nervous system. . In our computational model, we will treat motor neurons as normal neurons and define the network’s output as the potential of these motor neurons. Moreover, we define the input variables’ values as the neuron potential of a separate set of sensory neurons. Thus, the potential of these sensory neurons cannot be affected by synapses, as it is determined solely by the values of the inputs. Consequently, sensory neurons can only have outgoing synapses but no incoming links. . As the input and output variable might be differently scaled than the neuron values inside the network, we re-scale the input and output signals by element-wise affine transformations. . Finally, we can write down the full model. Let’s say X are the inputs of the network and Y are the outputs of the network. . Is this really used in neuroscience? . The model introduce above does not originate from our imagination, but is actually taken from the book Methods in Neuronal Modeling - From Ions to Networks 3 by Christof Koch and Idan Segev and has been used in neuroscience research4]. There is one difference though between our model and the one defined in the neuroscience book. The model described by Koch and Segev models each synapse also via a differential equation, i.e., giving every synapse a possibly different temporal behavior. Our modification replaces the synapse ODEs by their steady-state solution for t→∞t rightarrow inftyt→∞. This simplification significantly reduces the memory requirement to store the RNN-state from squared to linear, while losing a bit of expressive power. We called the resulting model the Liquid time constant (LTC) model. Why we named it that way and mathematical properties about it can be found in our arXiv preprint5. . References . Abbott 1999. Lapicque’s introduction of the integrate-and-fire model neuron (1907) &#8617; . | Hodgkin-Huxley Model &#8617; . | Koch and Segev Methods in Neuronal Modeling - From Ions to Networks &#8617; . | Wicks et al 1996. A Dynamic Network Simulation of the Nematode Tap Withdrawal Circuit: Predictions Concerning Synaptic Function Using Behavioral Criteria &#8617; . | Liquid Time-constant Networks &#8617; . |",
            "url": "https://mlech26l.github.io/pages/2020/09/14/the_model.html",
            "relUrl": "/2020/09/14/the_model.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "We won the 7th F1/TENTH Autonomous Grand Prix",
            "content": "Our team won first place in the 7th F1/TENTH Autonomous Grand Prix at the World Congress of the International Federation of Automatic Control (IFAC 2020). . . The F1/TENTH Grand Prix . F1/TENTH is a project about designing, building, and racing with autonomous cars at the scale of 1:10 of formula-1 cars (thus the name F1/TENTH). Initially founded by researchers from the University of Pennsylvania, F1/TENTH has attracted an international community and is regularly holding racing competitions. The latest of these competitions, the 7th F1/TENTH Autonomous Grand Prix, was indented to be held physically in Berlin. However, due to Covid-19, the race was carried out virtually in a simulation environment. I and students from TU Wien participated in this Grand Prix under the team name “TUfast TUfurious”. Out of a total of 13 submitted teams, including teams from, South Korea, U.S., and Italy, our team won the competition. . Our approach . Our agent implements a path following algorithm based on the race map, which the organizers released a week before the competition. We tested our agent with several pre-computed paths, including a very smooth one and one that drives the curves very aggressively. Eventually, we settled for a path, as shown below (right), that comprises of long straight parts where the agent can accelerate. . . After we fixed our path following core, we developed a velocity controller module. Based on the car’s current position, our velocity controller looks ahead of the planned path and estimates how much steering the agent has to do in the next couple of seconds. If the agent is expected to steer a lot, the velocity controller will reduce the car’s speed. Vice versa, if the track ahead is a straight part, the velocity controller will accelerate the vehicle. How far the velocity module looks ahead and how much to brake and accelerate in which conditions, is optimized using reinforcement learning. As for the learning part, we opt for a good-old random search algorithm. On top of the path following algorithm and the velocity controller, we implemented two methods that gave us an edge during the race. Our first improvement is a manual starting maneuver. Essentially, at the beginning of the race, we overwrite the velocity controller by accelerating with the maximum throttle. After a few seconds, we assign the control back to the learned velocity module. Our second improvement is a collision avoidance procedure. Our assumption the collision avoidance was that the opponents behave either near-optimal. Thus a safe overtaking during the race is not possible. Consequently, if our agent foresees colliding into the other car, for instance, because the other car slows down before a corner, our collision avoidance procedure slows down our car as well. . After we implemented all parts of our agent, we rigorously tested it. For instance, in the animation, we compared our agent to an opponent who performs a near-optimal start but then slows down halfway through the track. . . Our agent correctly foresees a crash a the beginning and slows down. After letting the opponent pass, our agent catches up quickly with the other car, when our agent has to slow down again to avoid a crash. . The competition . During the head-to-head races, our agent demonstrated that it performs remarkably well in a variety of scenarios. . . For instance, in the first race, our agent quickly overtook the opponent independently of starting in the advantaged or disadvantaged starting position. In the second race, our agent correctly avoided a crash with the fast-starting opponent. Nonetheless, our agent won due to a faster lap time. In the final race, our agent won because the other team’s car crashed into our agent, demonstrating how vital a well-tested collision avoidance is. All-in-all our agent was able to win because it performed excellently in a diverse set of conditions. . Acknowledgement . I want to thank all of my team members, Thomas Pintaric, Bernhard Schögl, Axel Brunnbauer, Hannes Brantner, and Andreas Brandstätter, for contributing to our submission and making the first-place possible. I also want to thank Radu Grosu for organizing the autonomous racing car course at TU Wien. . .",
            "url": "https://mlech26l.github.io/pages/2020/07/20/f1tenth.html",
            "relUrl": "/2020/07/20/f1tenth.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "The dangers of biases in AI",
            "content": "Background . At the 2020 Conference on Computer Vision and Pattern Recognition (CVPR) Duke University published a new method title Photo Upsampling via Latent Space Exploration (PULSE) for upsampling low resolution photos. In a nutshell, PULSE searches the latent space of a generative model to find a high-resolution image that downsamples to an image that looks similar to the low-resolution input image. In particular, the authors achieved fascinating looking results by using StyleGAN trained on the FFHQ dataset of high-resolution images of human faces. . Indeed, if we run PULSE with downsampled images generated by StyleGAN itself, we get remarkable detailed results: . . Nonetheless, the method has sparked extensive discussions about biases in AI. It all started with a tweet showing how PULSE upscaled a low-res picture of Barack Obama to a tanned white person. . 🤔🤔🤔 pic.twitter.com/LG2cimkCFm . &mdash; Chicken3gg (@Chicken3gg) June 20, 2020 The authors explicitly state that the objective of PULSE is not to reconstruct the original image, as this is impossible due to the lack of information in the low-res image. Nonetheless, one would never expect the low-res image of Barack Obama to be upscaled to the picture shown above. . The authors claim that biased training data of StyleGAN most likely causes the observed bias. The used FFHQ dataset primarily comprises of faces of white people. They hypothesize that, due to this training data imbalance, the explored parts of the latent space by PULSE correspond predominately to white people’s faces. . The dangers of biased AI . Despite this claimed lack of training data diversity, what’s fascinating and dangerous at the same time is how this bias manifests in the AI. To examine this, let’s downsample the Dancing Pallbearers and see what images PULSE will generate. . . While the synthetic low-res images share color and shape profile of the corresponding real picture, we notice a crucial property in each upsample image: It’s always a white person. Surprisingly, the AI comes up with subtle reasons for explaining the skin tone and head covering in the input image. The skin tone is not caused by dark skin color but because the person in the synthetic image stands in a shaded area. The white stripes on the hats are explained by sun rays hitting on that part of the person’s face. Moreover, the black area from the top hat in the first image is explained by the high volume of the person’s hair. . We see that the AI finds creative ways to explain the features observed in the test images that weren’t there during training. This is precisely where the danger of biased AI systems lie. The AI can find solutions that are valid on a mathematical level (L2-distance) but unacceptable on a societal level. We can repeat the experiments, but each time PULSE comes up with the same explanations. . . Bias beyond mathematics . The authors of the original paper performed an additional experiment to evaluate the &#39;’success rate’‘ of PULSE of various groups (Female/Male, Black, East Asian, Indian, etc. ). Their definition of success only concerns if the method finds an image that downsamples to a similar-looking image as the low-resolution input with respect to a pixel-based distance metric or not. The authors found a marginal but non-significant difference in ‘‘success rate’’ among the tested groups. Essentially, as I stated above, on a mathematical level, the authors were unable to characterize a significant bias in the system. . Conclusion . Bias in machine learning is a sensitive topic that cannot be reduced to mathematics and training data imbalance alone. It mandates examinations involving societal and domain-specific experts. .",
            "url": "https://mlech26l.github.io/pages/2020/06/30/biases.html",
            "relUrl": "/2020/06/30/biases.html",
            "date": " • Jun 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "5 Myths about quantized neural networks",
            "content": "Background: What are quantized neural networks? . When we run numerical algorithms on our computer, we need to make sacrifices in terms of precision for the sake of runtime. For instance, the square root of 2 is an irrational number and has an infinite amount of decimal digits. Thus we need to decide how many digits we really need for our application. Each extra digit of precision increases the memory and time requirements to store and compute a variable. . For example, the IEEE-754 standard specifies four types of floating-point formats: . np.sqrt(2): float128: 1.4142135623730950488 float64: 1.4142135623730951 float32: 1.4142135 float16: 1.414 . For machine learning applications, the float32 format has been the default choice, as it provides a decent performance while avoiding extreme numerical errors. However, in the past decade researcher have made the following two observations: . During the training phase, certain types of layers can be run and trained with lower precision (e.g., float16) | After the training phase (=inference phase), neural networks can run with much lower precision levels without sacrificing much accuracy | . Consequently, Nvidia&#39;s latest A100 GPU supports the following six numerical format: . Data type Significand precision Exponent . 0 float64 | 52-bits | 11-bits | . 1 float32 | 23-bits | 8-bits | . 2 TensorFloat32 | 10-bits | 8-bits | . 3 float16 | 10-bits | 5-bits | . 4 bfloat16 | 7-bits | 8-bits | . 5 int8 | 7-bits | 0-bits | . One item that stands out in this list is the last row: While all other formats are based on a floating-point representation, int8 is an integer type. . This raises the question: . How can we run a neural network with integer operations? . The answer is quantization. . Quantization translates a network that operates over floating-point variables into a network that uses fixed-point arithmetic . Fixed-point arithmetic is a numerical format that can be implemented relatively efficiently used integer operations.For instance, we can use the first four bits of an int8 value to represent the digits before the comma, and the last four bits to represent fractional digits that come after the comma: . Decimal: 0.5 + 1.25 = 1.75 Binary: 0000.1000 + 0001.0100 = 0001.1100 . A fixed-point addition can be implemented by simple integer addition and a fixed-point multiplication by an integer multiplication followed by a bit-wise shift operation. . Obviously, the precision achieved with an 8-bit fixed-point format is not enough for training a neural network. However, most types of layers can be quantized for inferencing without suffering a significant loss in accuracy. The quantization step itself rounds the float32 weight values to their nearest corresponding fixed-point value. . The clear advantages of running a network using int8 is that: . It requires less memory, which improves cache and memory bandwidth efficiency. | Can run using more efficient integer operations | In particular, a 2017 Google paper writes: . Eight-bit integer multiplies can be 6X less energy and 6X less area than IEEE 754 16-bit floating-point multiplies and the advantage for integer addition is 13X in energy and 38X in area [Dal16]. . - &#39;&#39;In-Datacenter Performance Analysis of a Tensor Processing Unit&#39;&#39; - Jouppi et al. . Despite this relatively simple concept, there are several misconceptions and myths regarding quantized neural networks: . Myth #1: Quantization is only necessary for ultra-low-power embedded systems . Far from it. Datacenter applications currently benefit the most from quantization. For instance, the first generation of Google&#39;s Tensor Processing Units (TPUs) only supported quantized networks. Computation units for floating-point arithmetic were only added in the second generation. . Likewise, Nvidia&#39;s V100 and latest A100 can perform four times as many int8 tensor operations compared to float32 operations per second (or twice as much int8 as float16 tensor operations per second). This means that you can quadruple the throughput of your datacenter application with quantization in a best-case scenario. . Myth #2: Quantization makes networks smaller but not faster . As already hinted in the myth above, modern AI accelerators such as GPU and TPU can run integer operations faster than floating-point operations. Let&#39;s look at the compute performance of Nvidia&#39;s latest A100 GPU: . Essentially, quantization does not only make the network smaller, but makes them also runs faster! . Myth #3: Any layer in a neural network can be quantized . Some types of layers do not tolerate quantization very well. For example, in a discussion of an ICLR paper by Max Welling&#39;s group we see that quantizing the first or the last layer of a network results in a considerable drop in accuracy. This gap does not entirely close even if we train the network using quantization-aware training techniques. . One trick often used to avoid this drop in accuracy is not to quantize the first and the last layer. As these two layers only take up a small fraction of the computations inside a network, running the first and the last layer with float32 does not hurt throughput much, but significantly benefits the accuracy of the network. . However, one some end-devices, this approach is not an option. For instance, Google&#39;s Edge TPU only supports int8. Therefore, in such cases, every layer of the network must be quantized to 8-bit integers. . Myth #4: It&#39;s easy to compare different quantization approaches . Comparing two different quantization methods is not a trivial job. Connecting to the discussion above, let&#39;s imagine we have a network and quantize it with two different methods to obtain network A and network B. While network A achieves a 90% accuracy by quantized all layers, network B achieves a 92% accuracy but leaves the first layer running with floating-point precision. . Which method is better? . The answer to this question depends on the context; which target device will the network run on? If it&#39;s a device without a floating-point unit such as the Edge TPU or a microcontroller, then method A is clearly better. Contrarily, if we plan to run the network on a V100 or A100 GPU, then method B might be the better approach. . Another technique that causes a lot of misconceptions found in the discussion of the ICLR paper by Max Welling&#39;s group are non-uniform quantization schemes: Fixed-point formats partition the representable value range using a uniform grid, e.g., there are the same amount of intermediate values between 1.5 and 2.5 as between 2.5 and 3.5. Looking at the typical weight distribution of neural networks, we notice that they follow a Gaussian-like bell curve distribution with smaller values occurring more frequently than large weight values. . If our only objective is to represent the weight values as accurately as possible using only 8-bits per parameter, then a logarithmic grid provides a better choice than the uniform partitioning of fixed-point formats. Although such non-uniform quantization approaches can remarkably compress the weights of the network, they destroy the critical advantage of quantization: The enabling of efficient integer arithmetic. . In essence, comparing different quantization methods can be quite challenging as there are many tradeoffs to be considered. . Myth #5: A 8-bit network uses only 8-bit variables . In an 8-bit quantized neural network, all weights and activations are 8-bit integer variables. However, when we run the network, we need 32-bits accumulation registers internally to represent intermediate results. . Let&#39;s take a look at the C-like implementation of a typical ReLU neuron: . float relu_neuron(float* w, float* x, size_t n){ float accumulator = 0; for(size_t i=0;i&lt;n;i++){ accumulator += w[i]*x[i]; } accumulator = max(0,accumulator); return accumulator; } . Essentially all variables, the weights, the inputs, and the accumulator, are represented by the 32-bit floating-point type. For a neural network in a fixed-point format, we need a wider integer accumulation register to perform the multiplications and summation. . int8_t relu_neuron(int8_t* w, int8_t* x, size_t n, int shift){ int32_t accumulator = 0; for(size_t i=0;i&lt;n;i++){ // Multiplying two 8-bit integers requires a 16-bit register int16_t product = w[i]*x[i]; // Sum over several 16-bit integers requires an even larger register (e.g. int32_t) accumulator += product; } // Fixed-point multiplication requires a shift accumulator = accumulator&gt;&gt;shift; accumulator = max(0,accumulator); // Project neuron value back to valid int8_t range accumulator = min(INT8_MAX,accumulator); // Cast 32-bit accumulator back to 8-bit integer return (int8_t)accumulator; } . Note that the parameter shift depends on the exact fixed-point format we employ, i.e., how many bits we use for storing the digits before and how many bits we use for storing the digits after the comma. . Page 27 of Nvidia&#39;s A100 whitepaper specifies precisely which numerical format uses which numerical type for representing the input operands and the accumulation registers. . Conclusion . Although the concept of neural network quantization is relatively simple, there are subtle details that can cause misconceptions. In this post, we have busted five common myths regarding quantized neural networks. .",
            "url": "https://mlech26l.github.io/pages/jupyter/quantization/2020/06/28/quantization.html",
            "relUrl": "/jupyter/quantization/2020/06/28/quantization.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The Teraflops era",
            "content": "OpenAI recently released a blog post, showing that the advances in algorithmic efficiency for training neural nets outpaced the scaling of Moore&#39;s law. In particular, the amount of transistors in silicon chips doubles every two years, whereas the efficiency of training a neural net to a certain accuracy level currently doubles every 16 to 17 months. While this progress is impressive, I am arguing that Moore&#39;s law is one of the main drivers of machine learning research. Thus, the advances in neural nets training efficiency are built on the shoulders of Moore&#39;s law. In essence, I am stating the following law: . The amount of machine learning experiments that can be done for a fixed budget doubles every two years . Moore&#39;s Law - Machine Learning Research Edition . By budget I am refering to time, money and compute resources. My definition of machine learning experiments involves any experiment to test new network designs, layers, loss functions, and training methods (e.g., self-supervised pre-training). . Machine Learning Research . If we look at the methodology of machine learning research, we notice an iterative paradigm composed of the following steps. . We have an idea | We test the idea | Based on the results, we refine and improve the idea. | Once this iterative process yields noteworthy results, the idea and corresponding test results get distilled into a research paper. . Let&#39;s say we want to speed up our research. Common sense tells us that we can speed up any process by getting rid of its bottlenecks. The most dominant bottleneck in the procedure above is obviously step number 2. . We could run more machine learning experiments if we simply buy a larger quantity of faster compute units. But what if our budget is limited? How can we speed up our research then? . The answer is simply waiting. Yes! Moore&#39;s law tells us that every 2 years, we get roughly twice the compute performance for the same budget. For instance, here is a plot of how the 32-bit floating-point of Nvidia GPU performance increased in the past decade: . We see that in terms of float32 performance, the RTX 3090 released in September of 2020 offers around 23x the float32 throughput of the GTX 580 release 9 years and 10 months prior. . Interestingly, this chart does not even include the improvements of mixed-precision methods and other tricks that achieve higher performance by sacrificing numerical precision. For instance, Nvidia&#39;s latest A100 can perform 156 Teraflop/s when using for machine learning dedicated TensorCores together with the slightly less precise TensorFloat32 numerical format. If we compare the TensorFloat32 throughput of the A100 to the GTX 580 used by Alex Krizhevsky to train AlexNet, we see a 100x scaling in compute performance in almost exactly ten years. . But what about larger models and datasets? . Of course, the statement about the doubling of the architecture tuning assumes that the datasets and extend of the networks does not change dramatically. However, given that ImageNet is still the de-facto standard computer vision benchmark (special credit to Prof. Fei-Fei), and the fact that the top-performing networks in 2019/2020 (EfficientNet) are smaller than their 2015/2016 counterparts (ResNet/ResNeXt), this assumption seems to hold for now. . Conclusion . Moore&#39;s law continues to make compute resources faster and cheaper. This increase in compute performance allows machine learning researchers to test a larger variety of new architecture and training methods. Therefore, I am expecting that research will continue to yield better neural archtectiures and training algorithms. .",
            "url": "https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html",
            "relUrl": "/jupyter/2020/06/08/gpu.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Distinguished Young Alumnus-Award",
            "content": "I won the Distinguished Young Alumnus-Award at the Epilog for my master thesis Brain-inspired Neural Control. The award was given by the TU Wien faculty of Informatics and is endowed 1.500 EUR. . . Remark: The research paper of my master thesis has been published at the 2019 IEEE International Conference on Robotics and Automation (ICRA) .",
            "url": "https://mlech26l.github.io/pages/2018/01/20/epilog.html",
            "relUrl": "/2018/01/20/epilog.html",
            "date": " • Jan 20, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Tikz and Videos",
            "content": "I am a really big fan of Vector graphics and I exhaustively use Tikz to draw such. However, creating videos and cool animations with Tikz and Latex can be a bit messy. So I have come up with a simple but effective hack to make generating videos with Latex easier. . The key idea is to have two separated Latex files: One that is manually created and stays the same over the whole video, and one that is procedurally generated for each frame. The manual one is created as you would do usually write your Latex image. However, all variables that are dynamic throughout the video (e.g. color, position, opacity, etc.) are defined by placeholders. And, guess what, the second Latex file (the procedurally generated one) then fills all the placeholder for each frame. . Drawing a frame . My starting point is the manually designed Latex file, that is based on the following scheme: . documentclass[border=0cm,convert={outext=.png}]{standalone} % Documentclass to directly create a PNG-files when invoking &#39;pdflatex&#39; usepackage{xcolor} usepackage{tikz} % Load dynamic variables input{dynamic.tex} begin{document} begin{tikzpicture} % My Latex code node (start) at (0,0) [draw,fill= mycolorsin] {A}; node (start) at (2,0) [draw,fill= mycolorcos] {B}; end{tikzpicture} end{document} . In this case a two nodes are drawn, which colors ( mycolor) should be animated. . . Filling the placeholders . In the next step I will create a simple python script that generates the dynamic.tex file and fills all the dynamic placeholder variables. Moreover, the program invokes the Latex compiler to generate a PNG file out of the code and stores it in a directory. . import numpy as np import time import os import subprocess import shutil # Create directory for the frames if not os.path.exists(&#39;sequence&#39;): os.makedirs(&#39;sequence&#39;) # Example data for animation in range [0,100] t = np.linspace(0,2*np.pi,100) sin_seq = 50.0*np.sin(t)+50.0 cos_seq = 50.0*np.cos(t)+50.0 # Loop over each frame for i in range(100): # Create file with for dynamic variables dynamic_file = open(&#39;dynamic.tex&#39;,&#39;w&#39;) dynamic_file.write(&#39; newcommand{ mycolorsin}{green!&#39;+str(int(round(sin_seq[i])))+&#39;!white} n&#39;) dynamic_file.write(&#39; newcommand{ mycolorcos}{red!&#39;+str(int(round(cos_seq[i])))+&#39;!white} n&#39;) dynamic_file.close() # Create png with pdflatex os.system(&#39;pdflatex -shell-escape -interaction=nonstopmode example.tex&#39;) # Move frame into directory with frames shutil.move(&#39;example.png&#39;, &#39;sequence/frame_&#39;+str(i).zfill(3)+&#39;.png&#39;) . Finally, all I have to do is to animate the frames with ImageMagick . convert -loop 0 -delay 2 sequence/*.png animation.gif . . Remarks . Instead of using newcommand for each placeholder, it might be more convinient to use tikzset or tikzstyle (See here) to define the dynamic variables | If you want to create a video instead of an animation (mp4 instead of gif), you can easily do this with ffmpeg | Technically the gif animation is not a vector graphics anymore. However by tuning the pdf-to-png conversion density parameter, one can create an animation with an arbitrary high resolution. | .",
            "url": "https://mlech26l.github.io/pages/2017/11/29/tikz_and_videos.html",
            "relUrl": "/2017/11/29/tikz_and_videos.html",
            "date": " • Nov 29, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am a PhD candidate at IST Austria at the research group of Tom Henzinger. My research topics are Machine Learning, Formal Methods, and Robotics. . Publications . ICML (2020) The Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits. Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu. | IJCAR (2020) An SMT Theory of Fixed-Point Arithmetic. Marek Baranowski, Shaobo He, Mathias Lechner, Thanh Son Nguyen, and Zvonimir Rakamaric [Paper] | ICLR (2020) Learning Representations for binary classification without backpropagation. Mathias Lechner. [Paper] | ICRA (2020) Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-To-End Robot Learning Scheme. Mathias Lechner, Ramin Hasani, Daniela Rus, and Radu Grosu. [Paper] | TACAS (2020) How Many Bits Does it Take to Quantize Your Neural Network?. Mirco Giacobbe, Thomas A. Henzinger, and Mathias Lechner [Paper] | ICRA (2019) Designing Worm-inspired Neural Networks for Interpretable Robotic Control. Mathias Lechner, Ramin Hasani, Manuel Zimmer, Thomas A. Henzinger, and Radu Grosu [Paper] | . Awards . Distinguished Young Alumnus-Award 2018 Faculty of Informatics at TU Wien .",
          "url": "https://mlech26l.github.io/pages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}