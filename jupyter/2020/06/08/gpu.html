<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>We live in a Teraflop era | Home</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="We live in a Teraflop era" />
<meta name="author" content="Mathias Lechner" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A post about how Moore’s law is driving progress in machine learning research" />
<meta property="og:description" content="A post about how Moore’s law is driving progress in machine learning research" />
<link rel="canonical" href="https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html" />
<meta property="og:url" content="https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html" />
<meta property="og:site_name" content="Home" />
<meta property="og:image" content="https://mlech26l.github.io/pages/images/gpu.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-08T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Mathias Lechner"},"description":"A post about how Moore’s law is driving progress in machine learning research","@type":"BlogPosting","headline":"We live in a Teraflop era","dateModified":"2020-06-08T00:00:00-05:00","datePublished":"2020-06-08T00:00:00-05:00","image":"https://mlech26l.github.io/pages/images/gpu.jpg","url":"https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/pages/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mlech26l.github.io/pages/feed.xml" title="Home" /><link rel="shortcut icon" type="image/x-icon" href="/pages/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>We live in a Teraflop era | Home</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="We live in a Teraflop era" />
<meta name="author" content="Mathias Lechner" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A post about how Moore’s law is driving progress in machine learning research" />
<meta property="og:description" content="A post about how Moore’s law is driving progress in machine learning research" />
<link rel="canonical" href="https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html" />
<meta property="og:url" content="https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html" />
<meta property="og:site_name" content="Home" />
<meta property="og:image" content="https://mlech26l.github.io/pages/images/gpu.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-08T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Mathias Lechner"},"description":"A post about how Moore’s law is driving progress in machine learning research","@type":"BlogPosting","headline":"We live in a Teraflop era","dateModified":"2020-06-08T00:00:00-05:00","datePublished":"2020-06-08T00:00:00-05:00","image":"https://mlech26l.github.io/pages/images/gpu.jpg","url":"https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlech26l.github.io/pages/jupyter/2020/06/08/gpu.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://mlech26l.github.io/pages/feed.xml" title="Home" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/pages/">Home</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/pages/about/">About Me</a><a class="page-link" href="/pages/search/">Search</a><a class="page-link" href="/pages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">We live in a Teraflop era</h1><p class="page-description">A post about how Moore&#39;s law is driving progress in machine learning research</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-08T00:00:00-05:00" itemprop="datePublished">
        Jun 8, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Mathias Lechner</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/pages/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-08-gpu.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>OpenAI recently released a <a href="https://openai.com/blog/ai-and-efficiency/">blog post</a>, showing that the advances in algorithmic efficiency for training neural nets outpaced the scaling of Moore's law. 
In particular, the amount of transistors in silicon chips <a href="https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf">doubles every two years</a>, whereas the efficiency of training a neural net to a certain accuracy level doubles every 16 to 17 months. 
While this progress is impressive, I am arguing that Moore's law is one of the main drivers of machine learning research. Thus, the advances in neural nets training efficiency are built on the shoulders of Moore's law.
In essence, I am stating the following law:</p>
<blockquote><p>The amount of architecture tuning that can be done for a fixed budget <strong>doubles</strong> every two years</p>
<p><strong><em>Moore's Law - Machine Learning Research Edition</em></strong></p>
</blockquote>
<p>By <em>budget</em> I am refering to time, money and compute resources. My definition of <em>architecture</em> involves any non-weight parameter/decision and includes the network design, hyperparameters, and training method as well, (e.g., self-supervised pre-training).</p>
<h2 id="Machine-Learning-Research">Machine Learning Research<a class="anchor-link" href="#Machine-Learning-Research"> </a></h2><p>If we look at the methodology of machine learning research, we notice an iterative paradigm composed of the following steps.</p>
<ol>
<li>We have an idea</li>
<li>We test the idea</li>
<li>Based on the results, we refine and improve the idea.</li>
</ol>
<p>Once this iterative process yields noteworthy results, the idea and corresponding test results get distilled into a research paper.</p>
<p>Let's say we want to speed up our research. 
Common sense tells us that we can speed up any process by getting rid of its bottlenecks.
The most dominant bottleneck in the procedure above is obviously step number 2.</p>
<p>We could run more machine learning experiments if we simply buy a larger quantity of faster compute units.
But what if our budget is limited? How can we speed up our research then?</p>
<p>The answer is simply waiting.
Yes! Moore's law tells us that every 2 years, we get roughly twice the compute performance for the same budget.
For instance, here is a plot of how the 32-bit floating-point of Nvidia GPU performance increased in the past decade:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

<div id="altair-viz-bff6319d7fc3476db42b13346ec17902"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-bff6319d7fc3476db42b13346ec17902") {
      outputDiv = document.getElementById("altair-viz-bff6319d7fc3476db42b13346ec17902");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "layer": [{"mark": {"type": "circle", "size": 60}, "encoding": {"color": {"type": "nominal", "field": "Gen"}, "tooltip": [{"type": "quantitative", "field": "Compute"}, {"type": "nominal", "field": "Node"}, {"type": "nominal", "field": "Gen"}, {"type": "nominal", "field": "Memory"}, {"type": "nominal", "field": "Tensor cores"}], "x": {"type": "temporal", "field": "Year", "scale": {"domain": ["2010-01-01T00:00:00", "2021-01-01T00:00:00"]}}, "y": {"type": "quantitative", "axis": {"title": "float32 Teraflop/s"}, "field": "Compute"}}, "height": 400, "width": 600}, {"mark": {"type": "text", "align": "left", "baseline": "middle", "dx": 7}, "encoding": {"color": {"type": "nominal", "field": "Gen"}, "text": {"type": "nominal", "field": "GPU"}, "tooltip": [{"type": "quantitative", "field": "Compute"}, {"type": "nominal", "field": "Node"}, {"type": "nominal", "field": "Gen"}, {"type": "nominal", "field": "Memory"}, {"type": "nominal", "field": "Tensor cores"}], "x": {"type": "temporal", "field": "Year", "scale": {"domain": ["2010-01-01T00:00:00", "2021-01-01T00:00:00"]}}, "y": {"type": "quantitative", "axis": {"title": "float32 Teraflop/s"}, "field": "Compute"}}, "height": 400, "width": 600}], "data": {"name": "data-3b7fa562e835dc8443afef5290c12238"}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-3b7fa562e835dc8443afef5290c12238": [{"GPU": "GTX 580", "Year": "2010-11-01T00:00:00", "Gen": "Fermi", "Memory": "1.5GB", "Compute": 1.581, "Tensor cores": false, "Node": "40nm"}, {"GPU": "GTX 680", "Year": "2012-02-01T00:00:00", "Gen": "Kepler", "Memory": "2GB", "Compute": 3.25, "Tensor cores": false, "Node": "28nm"}, {"GPU": "K40", "Year": "2013-10-01T00:00:00", "Gen": "Kepler", "Memory": "12GB", "Compute": 5.046, "Tensor cores": false, "Node": "28nm"}, {"GPU": "Titan Black", "Year": "2014-02-01T00:00:00", "Gen": "Kepler", "Memory": "6GB", "Compute": 5.645, "Tensor cores": false, "Node": "28nm"}, {"GPU": "K80", "Year": "2014-11-01T00:00:00", "Gen": "Kepler", "Memory": "2x12GB", "Compute": 8.226, "Tensor cores": false, "Node": "28nm"}, {"GPU": "GTX 980 Ti", "Year": "2015-06-01T00:00:00", "Gen": "Maxwell", "Memory": "6GB", "Compute": 6.06, "Tensor cores": false, "Node": "28nm"}, {"GPU": "M40", "Year": "2015-11-01T00:00:00", "Gen": "Maxwell", "Memory": "12GB", "Compute": 6.844, "Tensor cores": false, "Node": "28nm"}, {"GPU": "GTX 1080", "Year": "2016-05-01T00:00:00", "Gen": "Pascal", "Memory": "8GB", "Compute": 8.873, "Tensor cores": false, "Node": "16nm"}, {"GPU": "P100", "Year": "2016-04-01T00:00:00", "Gen": "Pascal", "Memory": "16GB", "Compute": 10.61, "Tensor cores": false, "Node": "16nm"}, {"GPU": "GTX 1080Ti", "Year": "2017-03-01T00:00:00", "Gen": "Pascal", "Memory": "11GB", "Compute": 11.34, "Tensor cores": false, "Node": "16nm"}, {"GPU": "Titan V", "Year": "2017-12-01T00:00:00", "Gen": "Volta", "Memory": "12GB", "Compute": 14.9, "Tensor cores": true, "Node": "12nm"}, {"GPU": "V100", "Year": "2018-03-01T00:00:00", "Gen": "Volta", "Memory": "16/32GB", "Compute": 14.13, "Tensor cores": true, "Node": "12nm"}, {"GPU": "RTX 2080 Ti", "Year": "2018-11-01T00:00:00", "Gen": "Turing", "Memory": "12GB", "Compute": 13.45, "Tensor cores": true, "Node": "12nm"}, {"GPU": "Titan RTX", "Year": "2018-12-01T00:00:00", "Gen": "Turing", "Memory": "24GB", "Compute": 16.31, "Tensor cores": true, "Node": "12nm"}, {"GPU": "A100", "Year": "2020-05-01T00:00:00", "Gen": "Ampere", "Memory": "40GB", "Compute": 19.49, "Tensor cores": true, "Node": "7nm"}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This chart does not even include the improvements of <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">mixed-precision methods</a> and other tricks that achieve higher performance by sacrificing numerical precision. For instance, Nvidia's latest A100 can perform 156 Teraflop/s when using for machine learning dedicated TensorCores together with the slightly less precise <a href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/">TensorFloat32</a> numerical format.
If we compare the TensorFloat32 throughput of the A100 to the GTX 580 used by Alex Krizhevsky to train <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>, we see a 100x scaling in compute performance in almost exactly ten years (or roughly 10x if we compare full-precision float32 performance).</p>
<h2 id="But-what-about-larger-models-and-datasets?">But what about larger models and datasets?<a class="anchor-link" href="#But-what-about-larger-models-and-datasets?"> </a></h2><p>Of course, the statement about the doubling of the architecture tuning assumes that the datasets and extend of the networks does not change dramatically.
However, given that ImageNet is still the de-facto standard computer vision benchmark (special credit to Prof. Fei-Fei), and the fact that the top-performing networks in 2019/2020 (<a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet</a>) are smaller than their 2015/2016 counterparts (<a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>/<a href="https://arxiv.org/pdf/1611.05431.pdf">ResNeXt</a>), this assumption seems to hold.</p>
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><p>Moore's law continues to make compute resources faster and cheaper. This increase in compute performance allows machine learning researchers to test a larger variety of new architecture and training methods. Therefore, I am expecting that research will continue to yield better neural archtectiures and training algorithms.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/pages/jupyter/2020/06/08/gpu.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/pages/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/pages/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/pages/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Personal research blog and webpage</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mlech26l" title="mlech26l"><svg class="svg-icon grey"><use xlink:href="/pages/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/MLech20" title="MLech20"><svg class="svg-icon grey"><use xlink:href="/pages/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
