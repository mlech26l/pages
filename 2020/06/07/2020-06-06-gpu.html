<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-6-08-gpu.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>OpenAI recently release a <a href="https://openai.com/blog/ai-and-efficiency/">blog post</a> showing that the advances in algorithmic efficiency for training neural nets outpaced the scaling of Moore's law. 
In particular, the amount of transistors in silicon chips doubles every 2 years, whereas the efficiency of training a neural net to a certain accuracy level doubles every 16 to 17 months. 
While this progess is impressive, I am arguing here that Moore's law is one of the main drivers in machine learning research. Thus, the advances in neural nets training efficiency is built on the shoulders of Moore's law.
In essence, I am stateing the following law:</p>
<blockquote><p>The amount of hyperparameter &amp; architecture tuning that can be done for a fixed budget <strong>doubles</strong> every 2 years</p>
<p><em>Moore's Law:Machine Learning Research Edition</em>
By <em>budget</em> I mean time, money and compute resources.</p>
</blockquote>
<h2 id="Machine-Learning-Research">Machine Learning Research<a class="anchor-link" href="#Machine-Learning-Research"> </a></h2><p>If we look at the methodology of machine learning research we notice an iterative paradigm composed of the following steps</p>
<ol>
<li>We have an idea</li>
<li>We test the idea</li>
<li>Based on the results we refine and improve the idea.</li>
</ol>
<p>Once this iterative process yields noteworthy results, the idea and corresponding test results get distilled into a research paper.</p>
<p>Let's say we want to speed up our research. 
Common sense tells us that we can speed up any process by getting rid of its bottlenecks.
The most domniant bottleneck in the procedure above is obviously step 2.</p>
<p>We could run more machine learning experiments if we simply buy a larger quanitity of faster compute units.
But what if our budget is limited? How can we speed up our research then?</p>
<p>The answer is, by simply waiting.
Yes! Moore's law tells us that in 2 years we get roughly twice the compute performance for the same budget.
For instance, here is a plot of how the 32-bit floating-point of Nvidia GPU performance increased in the past decade:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

<div id="altair-viz-bff6319d7fc3476db42b13346ec17902"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-bff6319d7fc3476db42b13346ec17902") {
      outputDiv = document.getElementById("altair-viz-bff6319d7fc3476db42b13346ec17902");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "layer": [{"mark": {"type": "circle", "size": 60}, "encoding": {"color": {"type": "nominal", "field": "Gen"}, "tooltip": [{"type": "quantitative", "field": "Compute"}, {"type": "nominal", "field": "Node"}, {"type": "nominal", "field": "Gen"}, {"type": "nominal", "field": "Memory"}, {"type": "nominal", "field": "Tensor cores"}], "x": {"type": "temporal", "field": "Year", "scale": {"domain": ["2010-01-01T00:00:00", "2021-01-01T00:00:00"]}}, "y": {"type": "quantitative", "axis": {"title": "float32 Teraflop/s"}, "field": "Compute"}}, "height": 400, "width": 600}, {"mark": {"type": "text", "align": "left", "baseline": "middle", "dx": 7}, "encoding": {"color": {"type": "nominal", "field": "Gen"}, "text": {"type": "nominal", "field": "GPU"}, "tooltip": [{"type": "quantitative", "field": "Compute"}, {"type": "nominal", "field": "Node"}, {"type": "nominal", "field": "Gen"}, {"type": "nominal", "field": "Memory"}, {"type": "nominal", "field": "Tensor cores"}], "x": {"type": "temporal", "field": "Year", "scale": {"domain": ["2010-01-01T00:00:00", "2021-01-01T00:00:00"]}}, "y": {"type": "quantitative", "axis": {"title": "float32 Teraflop/s"}, "field": "Compute"}}, "height": 400, "width": 600}], "data": {"name": "data-3b7fa562e835dc8443afef5290c12238"}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-3b7fa562e835dc8443afef5290c12238": [{"GPU": "GTX 580", "Year": "2010-11-01T00:00:00", "Gen": "Fermi", "Memory": "1.5GB", "Compute": 1.581, "Tensor cores": false, "Node": "40nm"}, {"GPU": "GTX 680", "Year": "2012-02-01T00:00:00", "Gen": "Kepler", "Memory": "2GB", "Compute": 3.25, "Tensor cores": false, "Node": "28nm"}, {"GPU": "K40", "Year": "2013-10-01T00:00:00", "Gen": "Kepler", "Memory": "12GB", "Compute": 5.046, "Tensor cores": false, "Node": "28nm"}, {"GPU": "Titan Black", "Year": "2014-02-01T00:00:00", "Gen": "Kepler", "Memory": "6GB", "Compute": 5.645, "Tensor cores": false, "Node": "28nm"}, {"GPU": "K80", "Year": "2014-11-01T00:00:00", "Gen": "Kepler", "Memory": "2x12GB", "Compute": 8.226, "Tensor cores": false, "Node": "28nm"}, {"GPU": "GTX 980 Ti", "Year": "2015-06-01T00:00:00", "Gen": "Maxwell", "Memory": "6GB", "Compute": 6.06, "Tensor cores": false, "Node": "28nm"}, {"GPU": "M40", "Year": "2015-11-01T00:00:00", "Gen": "Maxwell", "Memory": "12GB", "Compute": 6.844, "Tensor cores": false, "Node": "28nm"}, {"GPU": "GTX 1080", "Year": "2016-05-01T00:00:00", "Gen": "Pascal", "Memory": "8GB", "Compute": 8.873, "Tensor cores": false, "Node": "16nm"}, {"GPU": "P100", "Year": "2016-04-01T00:00:00", "Gen": "Pascal", "Memory": "16GB", "Compute": 10.61, "Tensor cores": false, "Node": "16nm"}, {"GPU": "GTX 1080Ti", "Year": "2017-03-01T00:00:00", "Gen": "Pascal", "Memory": "11GB", "Compute": 11.34, "Tensor cores": false, "Node": "16nm"}, {"GPU": "Titan V", "Year": "2017-12-01T00:00:00", "Gen": "Volta", "Memory": "12GB", "Compute": 14.9, "Tensor cores": true, "Node": "12nm"}, {"GPU": "V100", "Year": "2018-03-01T00:00:00", "Gen": "Volta", "Memory": "16/32GB", "Compute": 14.13, "Tensor cores": true, "Node": "12nm"}, {"GPU": "RTX 2080 Ti", "Year": "2018-11-01T00:00:00", "Gen": "Turing", "Memory": "12GB", "Compute": 13.45, "Tensor cores": true, "Node": "12nm"}, {"GPU": "Titan RTX", "Year": "2018-12-01T00:00:00", "Gen": "Turing", "Memory": "24GB", "Compute": 16.31, "Tensor cores": true, "Node": "12nm"}, {"GPU": "A100", "Year": "2020-05-01T00:00:00", "Gen": "Ampere", "Memory": "40GB", "Compute": 19.49, "Tensor cores": true, "Node": "7nm"}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This chart does not even include the improvements of Mixed-precision methods and other tricks that achieve a higher performance by sacreficing arithmetic precision. For instance, Nvidia's latest A100 can perform 156 Teraflop/s when using the slightly less precise TensorFloat32 format.
If we compare the TensorFloat32 throughput of the A100 to the GTX 580 used by Alex Krizhevsky to train AlexNet, we see a 100x jump in compute performance in almost exactly 10 years.</p>
<h2 id="But-what-about-larger-models-and-datasets?">But what about larger models and datasets?<a class="anchor-link" href="#But-what-about-larger-models-and-datasets?"> </a></h2><p>Of course the statement about the doubling of the hyperparameter &amp; architecture tuning assumes that the datasets and extend of the networks does not change dramatically.
However, given that ImageNet is still the de-facto standard computer vision benchmark (special credit to Prof. Fei-Fei), and the fact that the top performing networks in 2020 (EfficientNet) are smaller than their 2016 counterparts (ResNet/ResNeXt), this assumptions holds.</p>

</div>
</div>
</div>
</div>

