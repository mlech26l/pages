<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://mlech26l.github.io/pages/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mlech26l.github.io/pages/" rel="alternate" type="text/html" /><updated>2020-06-28T10:16:18-05:00</updated><id>https://mlech26l.github.io/pages/feed.xml</id><title type="html">Home</title><subtitle>Personal research blog and webpage</subtitle><entry><title type="html">5 Myths about quantized neural networks</title><link href="https://mlech26l.github.io/pages/jupyter/quantization/2020/06/27/quantization.html" rel="alternate" type="text/html" title="5 Myths about quantized neural networks" /><published>2020-06-27T00:00:00-05:00</published><updated>2020-06-27T00:00:00-05:00</updated><id>https://mlech26l.github.io/pages/jupyter/quantization/2020/06/27/quantization</id><content type="html" xml:base="https://mlech26l.github.io/pages/jupyter/quantization/2020/06/27/quantization.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-6-28-quantization.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Background:-What-are-quantized-neural-networks?&quot;&gt;Background: What are quantized neural networks?&lt;a class=&quot;anchor-link&quot; href=&quot;#Background:-What-are-quantized-neural-networks?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;When we run numerical algorithms on our computer we need to make sacrefices in terms of precision for the sake of runtime. 
For instance, the square root of 2 is an irrational number and has an infinite amount of decimal digits. 
Thus we need to decide how many digits we really need for our application.
Each additional increases the memory and time requirements to store and compute a variable.&lt;/p&gt;
&lt;p&gt;For example, the IEEE-754 standard specifies four types of floaint-point formats:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;np.sqrt(2):&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;float128:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;float64: &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;float32: &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;float16: &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;np.sqrt(2):
float128: 1.4142135623730950488
float64:  1.4142135623730951
float32:  1.4142135
float16:  1.414
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For machine learning applications, the &lt;code&gt;float32&lt;/code&gt; format has been the default choice, as it provides a decent performance while avoiding extreme numerical errors. 
However, in the past decade researcher have made the following two observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During the training phase, certain types of layers can be run and trained with lower precision level (e.g. &lt;code&gt;float16&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;After the training phase (=inference phase), neural networks can run with much lower precision levels without sacreficing much accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consequently, Nvidia's latest &lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf&quot;&gt;A100 GPU&lt;/a&gt; supports the following six numerical format:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Data type&lt;/th&gt;
      &lt;th&gt;Comment&lt;/th&gt;
      &lt;th&gt;Significand precision&lt;/th&gt;
      &lt;th&gt;Exponent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;float64&lt;/td&gt;
      &lt;td&gt;Double precision IEE-754 floating-point&lt;/td&gt;
      &lt;td&gt;52-bits&lt;/td&gt;
      &lt;td&gt;11-bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;float32&lt;/td&gt;
      &lt;td&gt;Single precision IEE-754 floating-point&lt;/td&gt;
      &lt;td&gt;23-bits&lt;/td&gt;
      &lt;td&gt;8-bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;TensorFloat32&lt;/td&gt;
      &lt;td&gt;32-bit floating-point format with reduced sign...&lt;/td&gt;
      &lt;td&gt;10-bits&lt;/td&gt;
      &lt;td&gt;8-bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;float16&lt;/td&gt;
      &lt;td&gt;Half precision IEE-754 floating-point&lt;/td&gt;
      &lt;td&gt;10-bits&lt;/td&gt;
      &lt;td&gt;5-bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;bfloat16&lt;/td&gt;
      &lt;td&gt;16-bit brain-float format with larger range bu...&lt;/td&gt;
      &lt;td&gt;7-bits&lt;/td&gt;
      &lt;td&gt;8-bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;int8&lt;/td&gt;
      &lt;td&gt;8-bit integer format for fixed-point arithmetic&lt;/td&gt;
      &lt;td&gt;7-bits&lt;/td&gt;
      &lt;td&gt;0-bits&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;One item that stands out in this list is the last row: While all other formats are based on a floating-point representation, &lt;code&gt;int8&lt;/code&gt; is an integer type.&lt;/p&gt;
&lt;p&gt;This raises the question:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;How can we run a neural network with integer operations?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The answer is quantization.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Important:Quantization turns a network that operates over floating-point variables into a network that uses fixed-point arithmetic
&lt;a href=&quot;https://en.wikipedia.org/wiki/Fixed-point_arithmetic&quot;&gt;Fixed-point arithmetic&lt;/a&gt; is a numerical format that can be implemented relatively effeciently used integer operations.
For instance, we can use the first four bits of an &lt;code&gt;int8&lt;/code&gt; value to represent the digits before the comma, and the last four bits to represent fractional digits that come after the comma:
```
0.5       + 1.25      = 1.75
0000.1000 + 0001.0100 = 0001.1100&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;```
A fixed-point addition can be implemented by simple integer addition and a fixed-point multiplication by a integer multiplication followed by an bit-wise shift operation.&lt;/p&gt;
&lt;p&gt;Obviously, the precision achieved with an 8-bit fixed-point format is not enough for training a neural network. However for inferencing, most types of layers can be quantized without suffering a significant loss in accuracy.
The quantization step itself rounds the &lt;code&gt;float32&lt;/code&gt; weight values to their nearest corresponding fixed-point value.&lt;/p&gt;
&lt;p&gt;The clear advantages of running a network using &lt;code&gt;int8&lt;/code&gt; is that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It requies less memory, which improves cache and memory bandwidth efficiency.&lt;/li&gt;
&lt;li&gt;Can run using more efficient integer operations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In particular, a &lt;a href=&quot;https://arxiv.org/pdf/1704.04760.pdf&quot;&gt;2017 Google paper&lt;/a&gt; writes:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Eight-bit integer multiplies can be 6X less energy and 6X less area than IEEE 754 16-bit floating-point multiplies, and the
advantage for integer addition is 13X in energy and 38X in area [Dal16].&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;- ''In-Datacenter Performance Analysis of a Tensor Processing Unit'' - Jouppi et al.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Despite this relatively simple concept, there are several misconceptions and myths regarding quantized neural networks:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Myth-#1:-Quantization-is-only-necessary-for-ultra-low-power-embedded-systems&quot;&gt;Myth #1: Quantization is only necessary for ultra-low-power embedded systems&lt;a class=&quot;anchor-link&quot; href=&quot;#Myth-#1:-Quantization-is-only-necessary-for-ultra-low-power-embedded-systems&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Far from it. Data center applications currently benefit the most from quantization. 
For instance, the first generation of &lt;a href=&quot;https://arxiv.org/pdf/1704.04760.pdf&quot;&gt;Google's Tensor Processing Units (TPUs)&lt;/a&gt; only supported 
quantized networks. Support for floating-point arithmetic was only added in the &lt;a href=&quot;https://www.tomshardware.com/news/tpu-v2-google-machine-learning,35370.html&quot;&gt;second generation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Likewise, Nvidia's &lt;a href=&quot;https://www.microway.com/knowledge-center-articles/in-depth-comparison-of-nvidia-tesla-volta-gpu-accelerators/&quot;&gt;V100&lt;/a&gt; and latest &lt;a href=&quot;https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products&quot;&gt;A100&lt;/a&gt; can
perform four times as many &lt;code&gt;int8&lt;/code&gt; tensor operations compared to &lt;code&gt;float32&lt;/code&gt; operations per second (or twice as much &lt;code&gt;int8&lt;/code&gt; as &lt;code&gt;float16&lt;/code&gt; tensor operations per second).
This means that, in a best case scenario, you can &lt;strong&gt;quadruple the throughput of your data center application&lt;/strong&gt;
if you quantize your network.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Myth-#2:-Quantization-makes-networks-smaller-but-not-faster&quot;&gt;Myth #2: Quantization makes networks smaller but not faster&lt;a class=&quot;anchor-link&quot; href=&quot;#Myth-#2:-Quantization-makes-networks-smaller-but-not-faster&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;As already hinted in the myth above, modern AI accelerators such as GPU and TPU can run integer operations faster than floating-point operations.
Let's have a look at the compute performance of Nvidia's latest &lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf&quot;&gt;A100 GPU&lt;/a&gt;:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;

&lt;div id=&quot;altair-viz-0929ad37219d472d8838efe1ef3d5e7c&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-0929ad37219d472d8838efe1ef3d5e7c&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-0929ad37219d472d8838efe1ef3d5e7c&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: &quot;bar&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Data type&quot;, &quot;legend&quot;: null, &quot;scale&quot;: {&quot;scheme&quot;: &quot;dark2&quot;}}, &quot;tooltip&quot;: [{&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Data type&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;TOPS&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Comment&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Significand precision&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Exponent&quot;}], &quot;x&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;axis&quot;: {&quot;labels&quot;: false, &quot;title&quot;: &quot;Data format&quot;}, &quot;field&quot;: &quot;Data type&quot;, &quot;sort&quot;: [&quot;float64&quot;, &quot;float32&quot;, &quot;TensorFloat32&quot;, &quot;float16&quot;, &quot;bfloat16&quot;, &quot;int8&quot;]}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;Tera op/s&quot;}, &quot;field&quot;: &quot;TOPS&quot;}}, &quot;height&quot;: 300, &quot;title&quot;: &quot;Nvidia A100 compute performance&quot;, &quot;width&quot;: 700}, {&quot;mark&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;align&quot;: &quot;center&quot;, &quot;baseline&quot;: &quot;middle&quot;, &quot;dy&quot;: -10, &quot;fontSize&quot;: 16}, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Data type&quot;, &quot;legend&quot;: null, &quot;scale&quot;: {&quot;scheme&quot;: &quot;dark2&quot;}}, &quot;text&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Data type&quot;}, &quot;tooltip&quot;: [{&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Data type&quot;}, {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;TOPS&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Comment&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Significand precision&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Exponent&quot;}], &quot;x&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;axis&quot;: {&quot;labels&quot;: false, &quot;title&quot;: &quot;Data format&quot;}, &quot;field&quot;: &quot;Data type&quot;, &quot;sort&quot;: [&quot;float64&quot;, &quot;float32&quot;, &quot;TensorFloat32&quot;, &quot;float16&quot;, &quot;bfloat16&quot;, &quot;int8&quot;]}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;Tera op/s&quot;}, &quot;field&quot;: &quot;TOPS&quot;}}, &quot;height&quot;: 300, &quot;title&quot;: &quot;Nvidia A100 compute performance&quot;, &quot;width&quot;: 700}], &quot;data&quot;: {&quot;name&quot;: &quot;data-150720172dd484d891e26d6e7b27f0a1&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-150720172dd484d891e26d6e7b27f0a1&quot;: [{&quot;Data type&quot;: &quot;float64&quot;, &quot;TOPS&quot;: 9.7, &quot;Significand precision&quot;: &quot;52-bits&quot;, &quot;Exponent&quot;: &quot;11-bits&quot;, &quot;Comment&quot;: &quot;Double precision IEE-754 floating-point&quot;}, {&quot;Data type&quot;: &quot;float32&quot;, &quot;TOPS&quot;: 19.5, &quot;Significand precision&quot;: &quot;23-bits&quot;, &quot;Exponent&quot;: &quot;8-bits&quot;, &quot;Comment&quot;: &quot;Single precision IEE-754 floating-point&quot;}, {&quot;Data type&quot;: &quot;TensorFloat32&quot;, &quot;TOPS&quot;: 156.0, &quot;Significand precision&quot;: &quot;10-bits&quot;, &quot;Exponent&quot;: &quot;8-bits&quot;, &quot;Comment&quot;: &quot;32-bit floating-point format with reduced significand precision&quot;}, {&quot;Data type&quot;: &quot;float16&quot;, &quot;TOPS&quot;: 312.0, &quot;Significand precision&quot;: &quot;10-bits&quot;, &quot;Exponent&quot;: &quot;5-bits&quot;, &quot;Comment&quot;: &quot;Half precision IEE-754 floating-point&quot;}, {&quot;Data type&quot;: &quot;bfloat16&quot;, &quot;TOPS&quot;: 312.0, &quot;Significand precision&quot;: &quot;7-bits&quot;, &quot;Exponent&quot;: &quot;8-bits&quot;, &quot;Comment&quot;: &quot;16-bit brain-float format with larger range but reduced significand precision&quot;}, {&quot;Data type&quot;: &quot;int8&quot;, &quot;TOPS&quot;: 624.0, &quot;Significand precision&quot;: &quot;7-bits&quot;, &quot;Exponent&quot;: &quot;0-bits&quot;, &quot;Comment&quot;: &quot;8-bit integer format for fixed-point arithmetic&quot;}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Essentially, quantization does not only make the network smaller, but it also runs faster!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Myth-#3:-Any-layer-in-a-neural-network-can-be-quantized&quot;&gt;Myth #3: Any layer in a neural network can be quantized&lt;a class=&quot;anchor-link&quot; href=&quot;#Myth-#3:-Any-layer-in-a-neural-network-can-be-quantized&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Some types of layers do not tolerate quantization very well. For example, in &lt;a href=&quot;https://openreview.net/forum?id=HkxjYoCqKX&amp;amp;noteId=rygmk1EDT7&quot;&gt;a discussion of an ICLR paper by Max Welling's group&lt;/a&gt; we see that quantizing the first or the last layer of a network results an a considerable drop in accuracy. 
This gap does not completely close even if we train the network using quantization-aware training.&lt;/p&gt;
&lt;p&gt;One trick which is often used to avoid this drop in accuracy is to not quantize the first and the last layer. 
As these two layers only take up a small fraction of the computations inside a network, running the first and the last layer with &lt;code&gt;float32&lt;/code&gt; does not hurt throughput much, but significantly benefits the accuracy of the network.&lt;/p&gt;
&lt;p&gt;However, one some end-devices this approach is not an option. For instance, &lt;a href=&quot;https://cloud.google.com/edge-tpu&quot;&gt;Google's Edge TPU&lt;/a&gt; only supports &lt;code&gt;int8&lt;/code&gt;. Therefore, in such cases every layer of the network must be quantized to 8-bit integers.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Myth-#4:-It's-easy-to-compare-different-quantization-approaches&quot;&gt;Myth #4: It's easy to compare different quantization approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Myth-#4:-It's-easy-to-compare-different-quantization-approaches&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Comparing two different quantization method is not a trivial job. 
Connecting to the discussion above, let's imagine we have a network and quantize it with two different methods to obtain network A and network B.
While, network A achieves a 90% accuracy by quantized all layers, network B achieves a 92% accuracy but leaves the first layer running with floating-point precision.&lt;/p&gt;
&lt;p&gt;Which method is better?&lt;/p&gt;
&lt;p&gt;The answer of this question depends on the context; which target device will the network run on?
If it's a device wihtout a floaing-point unit such as the Edge TPU or a microcontroller, then method A is clearly better. 
Contrarily, if we plan to run the network on a V100 or A100 GPU, then method B might be the better approach.&lt;/p&gt;
&lt;p&gt;Another technique that causes a lot of misconceptions found in &lt;a href=&quot;https://openreview.net/forum?id=HkxjYoCqKX&amp;amp;noteId=rygmk1EDT7&quot;&gt;the discussion of the ICLR paper by Max Welling's group&lt;/a&gt; are &lt;strong&gt;non-uniform quantization schemes&lt;/strong&gt;:
Fixed-point formats partition the representable value range using a uniform grid, e.g., there are the same amount of intermediate values between 1.5 and 2.5 as between 2.5 and 3.5. 
Looking at the typical weight distribution of neural networks, we notice that they follow a Gaussian-like bell curve distribution with smaller values occurcing more frequently than large weight values.&lt;/p&gt;
&lt;p&gt;If our sole purpose is to represent the weight values as accurately as possible using only 8-bits per parameter, then a logarithmic grid provides a better choice than the uniform partitioning of fixed-point formats.
Although, such non-uniform quantization approaches can compress the weights of the network remarkably well, they destroy the key advantage of quantization: The use of integer arithemtic.&lt;/p&gt;
&lt;p&gt;In essence, comparing quantization method can be quite challenging as there are many tradeoffs to be considered.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Myth-#5:-A-8-bit-network-uses-only-8-bit-variables&quot;&gt;Myth #5: A 8-bit network uses only 8-bit variables&lt;a class=&quot;anchor-link&quot; href=&quot;#Myth-#5:-A-8-bit-network-uses-only-8-bit-variables&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Mathias Lechner</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlech26l.github.io/pages/images/gpu.jpg" /><media:content medium="image" url="https://mlech26l.github.io/pages/images/gpu.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Moore’s Law - Machine Learning Research Edition</title><link href="https://mlech26l.github.io/pages/jupyter/2020/06/27/gpu.html" rel="alternate" type="text/html" title="Moore's Law - Machine Learning Research Edition" /><published>2020-06-27T00:00:00-05:00</published><updated>2020-06-27T00:00:00-05:00</updated><id>https://mlech26l.github.io/pages/jupyter/2020/06/27/gpu</id><content type="html" xml:base="https://mlech26l.github.io/pages/jupyter/2020/06/27/gpu.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-6-08-gpu.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;OpenAI recently released a &lt;a href=&quot;https://openai.com/blog/ai-and-efficiency/&quot;&gt;blog post&lt;/a&gt;, showing that the advances in algorithmic efficiency for training neural nets outpaced the scaling of Moore's law. 
In particular, the amount of transistors in silicon chips &lt;a href=&quot;https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf&quot;&gt;doubles every two years&lt;/a&gt;, whereas the efficiency of training a neural net to a certain accuracy level doubles every 16 to 17 months. 
While this progress is impressive, I am arguing that Moore's law is one of the main drivers of machine learning research. Thus, the advances in neural nets training efficiency are built on the shoulders of Moore's law.
In essence, I am stating the following law:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The amount of hyperparameter tuning that can be done for a fixed budget &lt;strong&gt;doubles&lt;/strong&gt; every two years&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Moore's Law - Machine Learning Research Edition&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By &lt;em&gt;budget&lt;/em&gt; I am refering to time, money and compute resources. My definition of &lt;em&gt;hyperparameters&lt;/em&gt; involves any non-weight parameter/decision and includes the architecture and training method as well, (e.g., self-supervised pre-training).&lt;/p&gt;
&lt;h2 id=&quot;Machine-Learning-Research&quot;&gt;Machine Learning Research&lt;a class=&quot;anchor-link&quot; href=&quot;#Machine-Learning-Research&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;If we look at the methodology of machine learning research, we notice an iterative paradigm composed of the following steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We have an idea&lt;/li&gt;
&lt;li&gt;We test the idea&lt;/li&gt;
&lt;li&gt;Based on the results, we refine and improve the idea.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once this iterative process yields noteworthy results, the idea and corresponding test results get distilled into a research paper.&lt;/p&gt;
&lt;p&gt;Let's say we want to speed up our research. 
Common sense tells us that we can speed up any process by getting rid of its bottlenecks.
The most dominant bottleneck in the procedure above is obviously step number 2.&lt;/p&gt;
&lt;p&gt;We could run more machine learning experiments if we simply buy a larger quantity of faster compute units.
But what if our budget is limited? How can we speed up our research then?&lt;/p&gt;
&lt;p&gt;The answer is simply waiting.
Yes! Moore's law tells us that every 2 years, we get roughly twice the compute performance for the same budget.
For instance, here is a plot of how the 32-bit floating-point of Nvidia GPU performance increased in the past decade:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;

&lt;div id=&quot;altair-viz-bff6319d7fc3476db42b13346ec17902&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-bff6319d7fc3476db42b13346ec17902&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-bff6319d7fc3476db42b13346ec17902&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;circle&quot;, &quot;size&quot;: 60}, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Gen&quot;}, &quot;tooltip&quot;: [{&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Compute&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Node&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Gen&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Memory&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Tensor cores&quot;}], &quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;Year&quot;, &quot;scale&quot;: {&quot;domain&quot;: [&quot;2010-01-01T00:00:00&quot;, &quot;2021-01-01T00:00:00&quot;]}}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;float32 Teraflop/s&quot;}, &quot;field&quot;: &quot;Compute&quot;}}, &quot;height&quot;: 400, &quot;width&quot;: 600}, {&quot;mark&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;align&quot;: &quot;left&quot;, &quot;baseline&quot;: &quot;middle&quot;, &quot;dx&quot;: 7}, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Gen&quot;}, &quot;text&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;GPU&quot;}, &quot;tooltip&quot;: [{&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;Compute&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Node&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Gen&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Memory&quot;}, {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;Tensor cores&quot;}], &quot;x&quot;: {&quot;type&quot;: &quot;temporal&quot;, &quot;field&quot;: &quot;Year&quot;, &quot;scale&quot;: {&quot;domain&quot;: [&quot;2010-01-01T00:00:00&quot;, &quot;2021-01-01T00:00:00&quot;]}}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;float32 Teraflop/s&quot;}, &quot;field&quot;: &quot;Compute&quot;}}, &quot;height&quot;: 400, &quot;width&quot;: 600}], &quot;data&quot;: {&quot;name&quot;: &quot;data-3b7fa562e835dc8443afef5290c12238&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-3b7fa562e835dc8443afef5290c12238&quot;: [{&quot;GPU&quot;: &quot;GTX 580&quot;, &quot;Year&quot;: &quot;2010-11-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Fermi&quot;, &quot;Memory&quot;: &quot;1.5GB&quot;, &quot;Compute&quot;: 1.581, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;40nm&quot;}, {&quot;GPU&quot;: &quot;GTX 680&quot;, &quot;Year&quot;: &quot;2012-02-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Kepler&quot;, &quot;Memory&quot;: &quot;2GB&quot;, &quot;Compute&quot;: 3.25, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;28nm&quot;}, {&quot;GPU&quot;: &quot;K40&quot;, &quot;Year&quot;: &quot;2013-10-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Kepler&quot;, &quot;Memory&quot;: &quot;12GB&quot;, &quot;Compute&quot;: 5.046, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;28nm&quot;}, {&quot;GPU&quot;: &quot;Titan Black&quot;, &quot;Year&quot;: &quot;2014-02-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Kepler&quot;, &quot;Memory&quot;: &quot;6GB&quot;, &quot;Compute&quot;: 5.645, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;28nm&quot;}, {&quot;GPU&quot;: &quot;K80&quot;, &quot;Year&quot;: &quot;2014-11-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Kepler&quot;, &quot;Memory&quot;: &quot;2x12GB&quot;, &quot;Compute&quot;: 8.226, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;28nm&quot;}, {&quot;GPU&quot;: &quot;GTX 980 Ti&quot;, &quot;Year&quot;: &quot;2015-06-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Maxwell&quot;, &quot;Memory&quot;: &quot;6GB&quot;, &quot;Compute&quot;: 6.06, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;28nm&quot;}, {&quot;GPU&quot;: &quot;M40&quot;, &quot;Year&quot;: &quot;2015-11-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Maxwell&quot;, &quot;Memory&quot;: &quot;12GB&quot;, &quot;Compute&quot;: 6.844, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;28nm&quot;}, {&quot;GPU&quot;: &quot;GTX 1080&quot;, &quot;Year&quot;: &quot;2016-05-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Pascal&quot;, &quot;Memory&quot;: &quot;8GB&quot;, &quot;Compute&quot;: 8.873, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;16nm&quot;}, {&quot;GPU&quot;: &quot;P100&quot;, &quot;Year&quot;: &quot;2016-04-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Pascal&quot;, &quot;Memory&quot;: &quot;16GB&quot;, &quot;Compute&quot;: 10.61, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;16nm&quot;}, {&quot;GPU&quot;: &quot;GTX 1080Ti&quot;, &quot;Year&quot;: &quot;2017-03-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Pascal&quot;, &quot;Memory&quot;: &quot;11GB&quot;, &quot;Compute&quot;: 11.34, &quot;Tensor cores&quot;: false, &quot;Node&quot;: &quot;16nm&quot;}, {&quot;GPU&quot;: &quot;Titan V&quot;, &quot;Year&quot;: &quot;2017-12-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Volta&quot;, &quot;Memory&quot;: &quot;12GB&quot;, &quot;Compute&quot;: 14.9, &quot;Tensor cores&quot;: true, &quot;Node&quot;: &quot;12nm&quot;}, {&quot;GPU&quot;: &quot;V100&quot;, &quot;Year&quot;: &quot;2018-03-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Volta&quot;, &quot;Memory&quot;: &quot;16/32GB&quot;, &quot;Compute&quot;: 14.13, &quot;Tensor cores&quot;: true, &quot;Node&quot;: &quot;12nm&quot;}, {&quot;GPU&quot;: &quot;RTX 2080 Ti&quot;, &quot;Year&quot;: &quot;2018-11-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Turing&quot;, &quot;Memory&quot;: &quot;12GB&quot;, &quot;Compute&quot;: 13.45, &quot;Tensor cores&quot;: true, &quot;Node&quot;: &quot;12nm&quot;}, {&quot;GPU&quot;: &quot;Titan RTX&quot;, &quot;Year&quot;: &quot;2018-12-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Turing&quot;, &quot;Memory&quot;: &quot;24GB&quot;, &quot;Compute&quot;: 16.31, &quot;Tensor cores&quot;: true, &quot;Node&quot;: &quot;12nm&quot;}, {&quot;GPU&quot;: &quot;A100&quot;, &quot;Year&quot;: &quot;2020-05-01T00:00:00&quot;, &quot;Gen&quot;: &quot;Ampere&quot;, &quot;Memory&quot;: &quot;40GB&quot;, &quot;Compute&quot;: 19.49, &quot;Tensor cores&quot;: true, &quot;Node&quot;: &quot;7nm&quot;}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This chart does not even include the improvements of &lt;a href=&quot;https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html&quot;&gt;mixed-precision methods&lt;/a&gt; and other tricks that achieve higher performance by sacrificing numerical precision. For instance, Nvidia's latest A100 can perform 156 Teraflop/s when using the slightly less precise &lt;a href=&quot;https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/&quot;&gt;TensorFloat32&lt;/a&gt; format.
If we compare the TensorFloat32 throughput of the A100 to the GTX 580 used by Alex Krizhevsky to train &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;AlexNet&lt;/a&gt;, we see a 100x scaling in compute performance in almost exactly ten years (or roughly 10x if we compare full-precision float32 performance).&lt;/p&gt;
&lt;h2 id=&quot;But-what-about-larger-models-and-datasets?&quot;&gt;But what about larger models and datasets?&lt;a class=&quot;anchor-link&quot; href=&quot;#But-what-about-larger-models-and-datasets?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Of course, the statement about the doubling of the hyperparameter tuning assumes that the datasets and extend of the networks does not change dramatically.
However, given that ImageNet is still the de-facto standard computer vision benchmark (special credit to Prof. Fei-Fei), and the fact that the top-performing networks in 2019/2020 (&lt;a href=&quot;https://arxiv.org/pdf/1905.11946.pdf&quot;&gt;EfficientNet&lt;/a&gt;) are smaller than their 2015/2016 counterparts (&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;ResNet&lt;/a&gt;/&lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;ResNeXt&lt;/a&gt;), this assumption seems to hold.&lt;/p&gt;
&lt;h2 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Moore's law continues to make compute resources faster and cheaper. This increase in compute performance allows machine learning researchers to test a larger variety of new architecture and training methods. Therefore, I am expecting that research will continue to yield better neural archtectiures and training algorithms.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Mathias Lechner</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlech26l.github.io/pages/images/gpu.jpg" /><media:content medium="image" url="https://mlech26l.github.io/pages/images/gpu.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Distinguished Young Alumnus-Award</title><link href="https://mlech26l.github.io/pages/2018/01/20/epilog.html" rel="alternate" type="text/html" title="Distinguished Young Alumnus-Award" /><published>2018-01-20T00:00:00-06:00</published><updated>2018-01-20T00:00:00-06:00</updated><id>https://mlech26l.github.io/pages/2018/01/20/epilog</id><content type="html" xml:base="https://mlech26l.github.io/pages/2018/01/20/epilog.html">&lt;p&gt;I won the &lt;strong&gt;Distinguished Young Alumnus-Award&lt;/strong&gt; at the  &lt;a href=&quot;http://www.informatik.tuwien.ac.at/studium/studierende/epilog/2017ws&quot;&gt;Epilog&lt;/a&gt;
for my master thesis &lt;em&gt;Brain-inspired Neural Control&lt;/em&gt;. The award was given by the TU Wien faculty of Informatics and is endowed 1.500 EUR.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pages/images/distinguished.jpg&quot; alt=&quot;award&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: 
The research paper of my master thesis has been published at the &lt;a href=&quot;https://ieeexplore.ieee.org/document/8793840&quot;&gt;2019 IEEE International Conference on Robotics and Automation (ICRA)&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="Award" /><category term="Epilog" /><category term="Distinguished Young Alumnus" /><summary type="html">I won the Distinguished Young Alumnus-Award at the Epilog for my master thesis Brain-inspired Neural Control. The award was given by the TU Wien faculty of Informatics and is endowed 1.500 EUR.</summary></entry><entry><title type="html">Tikz and Videos</title><link href="https://mlech26l.github.io/pages/2017/11/29/tikz_and_videos.html" rel="alternate" type="text/html" title="Tikz and Videos" /><published>2017-11-29T00:00:00-06:00</published><updated>2017-11-29T00:00:00-06:00</updated><id>https://mlech26l.github.io/pages/2017/11/29/tikz_and_videos</id><content type="html" xml:base="https://mlech26l.github.io/pages/2017/11/29/tikz_and_videos.html">&lt;p&gt;I am a really big fan of &lt;strong&gt;Vector graphics&lt;/strong&gt; and I exhaustively use &lt;a href=&quot;https://en.wikipedia.org/wiki/PGF/TikZ&quot;&gt;Tikz&lt;/a&gt; to draw such.
However, creating videos and cool animations with Tikz and Latex can be a bit messy.
So I have come up with a simple but effective hack to make generating videos with Latex easier.&lt;/p&gt;

&lt;p&gt;The key idea is to have two separated Latex files: One that is manually created and stays the same over the whole video, and one that is procedurally generated for each frame.
The manual one is created as you would do usually write your Latex image. However, all variables that are dynamic throughout the video (e.g. color, position, opacity, etc.) are defined by placeholders. And, guess what, the second Latex file (the procedurally generated one) then fills all the placeholder for each frame.&lt;/p&gt;

&lt;h1 id=&quot;drawing-a-frame&quot;&gt;Drawing a frame&lt;/h1&gt;
&lt;p&gt;My starting point is the manually designed Latex file, that is based on the following scheme:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-tex&quot; data-lang=&quot;tex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\documentclass&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[border=0cm,convert={outext=.png}]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;standalone&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Documentclass to directly create a PNG-files when invoking 'pdflatex'&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;xcolor&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;tikz&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% Load dynamic variables&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;dynamic.tex&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\begin{document}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{tikzpicture}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;% My Latex code&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (start) at (0,0) [draw,fill=&lt;span class=&quot;k&quot;&gt;\mycolorsin&lt;/span&gt;] &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;A&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;;
&lt;span class=&quot;k&quot;&gt;\node&lt;/span&gt; (start) at (2,0) [draw,fill=&lt;span class=&quot;k&quot;&gt;\mycolorcos&lt;/span&gt;] &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;B&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;;

&lt;span class=&quot;nt&quot;&gt;\end{tikzpicture}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{document}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this case a two nodes are drawn, which colors (&lt;em&gt;\mycolor&lt;/em&gt;) should be animated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pages/images/tikz/frame_000.png&quot; alt=&quot;Latex&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;filling-the-placeholders&quot;&gt;Filling the placeholders&lt;/h1&gt;

&lt;p&gt;In the next step I will create a simple &lt;em&gt;python&lt;/em&gt; script that generates the &lt;em&gt;dynamic.tex&lt;/em&gt; file and fills all the dynamic placeholder variables.
Moreover, the program invokes the Latex compiler to generate a PNG file out of the code and stores it in a directory.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;subprocess&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;shutil&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create directory for the frames
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sequence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sequence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Example data for animation in range [0,100]
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sin_seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cos_seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Loop over each frame
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Create file with for dynamic variables
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dynamic_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dynamic.tex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dynamic_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;newcommand{&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mycolorsin}{green!'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'!white}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dynamic_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;newcommand{&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mycolorcos}{red!'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'!white}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dynamic_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Create png with pdflatex
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pdflatex -shell-escape -interaction=nonstopmode example.tex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Move frame into directory with frames
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;shutil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'example.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sequence/frame_'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zfill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Finally, all I have to do is to animate the frames with &lt;a href=&quot;https://en.wikipedia.org/wiki/ImageMagick&quot;&gt;ImageMagick&lt;/a&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;convert &lt;span class=&quot;nt&quot;&gt;-loop&lt;/span&gt; 0 &lt;span class=&quot;nt&quot;&gt;-delay&lt;/span&gt; 2 sequence/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.png animation.gif&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/pages/images/tikz/animation.gif&quot; alt=&quot;Latex&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;remarks&quot;&gt;Remarks&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Instead of using &lt;em&gt;\newcommand&lt;/em&gt; for each placeholder, it might be more convinient to use &lt;em&gt;tikzset&lt;/em&gt; or &lt;em&gt;tikzstyle&lt;/em&gt; (&lt;a href=&quot;https://tex.stackexchange.com/questions/52372/should-tikzset-or-tikzstyle-be-used-to-define-tikz-styles&quot;&gt;See here&lt;/a&gt;) to define the dynamic variables&lt;/li&gt;
  &lt;li&gt;If you want to create a video instead of an animation (mp4 instead of gif), you can easily do this with &lt;a href=&quot;https://en.wikipedia.org/wiki/FFmpeg&quot;&gt;ffmpeg&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Technically the gif animation is not a vector graphics anymore. However by tuning the &lt;em&gt;pdf-to-png&lt;/em&gt; conversion density parameter, one can create an animation with an arbitrary high resolution.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Tikz" /><category term="Latex" /><category term="Animation" /><category term="Video" /><summary type="html">I am a really big fan of Vector graphics and I exhaustively use Tikz to draw such. However, creating videos and cool animations with Tikz and Latex can be a bit messy. So I have come up with a simple but effective hack to make generating videos with Latex easier.</summary></entry></feed>